<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Deep Spiking Convolutional Neural Network for Single Object Localization Based On Deep Continuous Local
		Learning</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/white.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<!-- Titre -->
			<section>
				<h5 style="text-transform: capitalize;">Deep Spiking Convolutional Neural Network for Single Object
					Localization Based On Deep Continuous Local Learning</h5>
				<br>
				<br>
				<small><b>Sami BARCHID</b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; José MENNESSON &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					Chaabane DJÉRABA</small>
				<img src="img/SNN_localization/logos_cbmi.png">
				<p><small id="js-current-date"></small></p>
				<script>
					document.getElementById('js-current-date').innerHTML = new Date().toLocaleDateString("en-US")
				</script>
			</section>

			<section>
				<section data-markdown>
					# Context
				</section>
				<section>
					<h3>Deep learning</h3>
					<ul>
						<li>State-of-the-art in almost all Computer Vision and Machine Learning tasks</li>
						<li>Growing computational, memory and energy costs</li>
					</ul>
					<img src="img/SNN_localization/carbon-footprint.png">
				</section>

				<section>
					<h3>Spiking Neural Networks <span
							style="text-transform:none; font-weight: normal; font-style: italic;">(SNNs)</span> </h3>
					<ul>
						<li>Third-generation of AI <small>[Maass1997]</small></li>
						<br>
						<li>Spiking neurons : strongly inspired by the neurons of our brain</li>
						<br>
						<li>Implementable on low-power neuromorphic hardware <small>[Davies2018]</small></li>
						<br>
						<li class="fragment"><b>A potential solution</b></li>
					</ul>
				</section>

				<section>
					<h3>Challenges</h3>
					<ul>
						<li>Performance still behind Artificial Neural Networks (ANNs)</li>
						<br>

						<li>Spiking neurons are not differentiable <small>[Kaiser2020]</small></li>
						<ul>
							<li class="fragment"><b>No backpropagation</b></li>
						</ul>
					</ul>
				</section>
			</section>

			<section>
				<section data-markdown>
					# SNN for Computer Vision
				</section>

				<!-- <section>
					<h3>TODO</h3>
				</section> -->

				<section>
					<h3>Image neural Coding</h3>

					<ul>
						<li><b>Definition : </b> converts the numerical values of pixels into spikes.</li>
						<li>No established methods</li>
					</ul>
					<img style="height: 325px" src="img/SNN_localization/frequency_coding.png" alt="Neural Coding examples">
					<small>Falez, Pierre. Improving spiking neural networks trained with spike timing dependent
						plasticity for image recognition. Diss. Université de Lille, 2019.</small>
				</section>

				<section>
					<h3>Definition</h3>

					<ul>
						<li>
							An SNN consists of <b>spiking neurons</b> that communicate asynchronously through discrete
							spatio-temporal events called <b>spikes</b>.
						</li>
						<li>
							Leaky Integrate and Fire (LIF) neuron model
						</li>
					</ul>

					<img
						class="fragment" src="img/SNN_localization/Spiking_neuron.png" style="height: 400px">
				</section>

				<section>
					<h3>Running an SNN</h3>
					<ul>
						<div class="fragment">
							<li> <b>Neuromorphic hardware</b></li>

							<ul>
								<li>Limited capacity of neurons</li>
								<li>Difficult to access</li>
							</ul>
						</div>
						<br>
						<div class="fragment">
							<li><b>Simulation on CPU/GPU</b></li>

							<ul>
								<li>The continuous dynamics of spiking neurons cannot be run 'as is'</li>
								<li>Need to discretize the continuous dynamics into $T$ timesteps </li>
							</ul>
						</div>
					</ul>
				</section>

				<section>
					<h3>Learning Approaches</h3>
					<ol>
						<div class="fragment">
							<b><li>Unsupervised local learning <small>[Caporale2008]</small>
							</li></b>
							<ul>
								<li>Neuromorphic implementation</li>
								<li>No annotation</li>
							</ul>
						</div>
						<div class="fragment">
							<b><li>ANN-to-SNN conversion <small>[Cao2015]</small></li></b>
							<ul>
								<li>Best performance</li>
								<li>Training is still on ANNs</li>
							</ul>
						</div>
						<div class="fragment">
							<div class="fragment grow">
								<b><li>Adapted backpropagation <small>[Kaiser2020]</small></li></b>
								<ul>
									<li>Good performance recently</li>
								</ul>
							</div>
						</div>
					</ol>
				</section>

				<section>
					<h3>State of SNN in Computer Vision</h3>
					<ul>
						<li>Simple recognition tasks <small>[Falez2019]</small> <i>(e.g. digit recognition)</i></li>
						<li>Mostly shallow networks</li>
						<li>Lack of works towards complex vision tasks</li>
						<br>
						<div class="fragment">
							<b>
								<li>Proposed contribution :</li>
							</b>
							<ul>
								<li>Exploit recent supervised learning approaches to perform complex vision task</li>
							</ul>
						</div>
					</ul>
				</section>
			</section>

			<section>
				<section>
					<h1>Deep Continuous Local Learning</h1>
					<p>(DECOLLE)</p>
					<br>
					<br>
					<small>Kaiser, et al. “Synaptic plasticity dynamics for
						deep continuous local learning (decolle),” Frontiers in Neuroscience, vol. 14, p. 424,
						2020</small>
				</section>

				<section data-markdown>
					### Overview
					- Supervised surrogate gradient learning
					- Online learning
						- Synaptic weights are updated at each time-step
					- Local error optimization
						- Implementable on neuromorphic computers
					- Low memory complexity
						- Enable deep network simulation
				</section>

				<section>
					<h3>Illustration</h3>
					<img src="img/SNN_localization/DECOLLE.png">
					<small>Kaiser, et al. “Synaptic plasticity dynamics for
						deep continuous local learning (decolle),” Frontiers in Neuroscience, vol. 14, p. 424,
						2020</small>
				</section>
			</section>

			<section>
				<section data-markdown>
					# Contributions
				</section>

				<section>
					<h3>Formulation</h3>
					<img src="img/SNN_localization/FPN_DECOLLE_Overview.png">
				</section>

				<!-- <section>
					<img src="img/frequency_coding.png">
					<small>Falez et al., "Improving Spiking Neural Networks Trained with Spike Timing Dependent
						Plasticity for Image Recognition"</small>
				</section> -->

				<section>
					<h3>Readout Layers</h3>
					<ul>
						<li>A readout layer is attached to each layer</li>
						<li>It produces a bounding box prediction</li>
						<br>
						<li class="fragment" data-fragment-index="1">A DCSNN $s(\mathbf{I})$ composed of $L$ layers outputs $L$ bounding box predictions</li>
					</ul>

					<p class="fragment" data-fragment-index="1">In short, &nbsp;&nbsp;&nbsp; $s(\mathbf{I}) = \{\mathbf{B}_i\}^{L}_1$</p>
				</section>

				<section>
					<h3>Architecture</h3>
					<img src="img/SNN_localization/architecture.png">
				</section>

				<section data-markdown>
					### Output Conversion Strategy
					- DECOLLE = online learning
						- An output prediction is produced for each of the `$T$` timesteps
					- One prediction for each readout layer
						- **Which prediction should we choose ?**
				</section>

				<section data-markdown>
					### Output Conversion Strategy
					- A rate-coded image delivers its total information after the `$T$` timesteps
						- **Keep the prediction of the last timestep**

					- Hierarchical representation of successive layers
						- **Keep the prediction of the last layer, `$\mathbf{B}_L$`**
				</section>
			</section>

			<section>
				<section>
					<h1>Experiments</h1>
				</section>
				<section>
					<h3>Oxford-IIIT-Pet dataset</h3>
					<ul>
						<li>$176 \times 240$ dimension</li>
						<li>Training split: <b>6000 images</b></li>
						<li>Testing split: <b>1349 images</b></li>
					</ul>
					<img src="img/SNN_localization/Oxford_IIIT_Pet.png">
				</section>
				
				<!-- <section>
					<h3>Mean Intersection over Union <span style="text-transform: none;">(mIoU)</span></h3>
						<p>The mean value of all the intersections between the predicted $\mathbf{B}$ and the ground truth over their unions.</p>
						<img src="img/SNN_localization/IoU.png" alt="IoU" style="height: 400px;">
					</ul>
				</section> -->

				<section>
					<h3>Results</h3>
					<p>$63.2$% mIoU</p>
					<div class="fragment">
						<img src="img/SNN_localization/qualitative_results.png">
						<ul>
							<li>Most object are well-localized ($55$ to $75$% mIoU)</li>
							<li>Small objects are poorly localized ($\le 15$% mIoU)</li>
						</ul>
					</div>
				</section>

				<section>
					<h3>Hypotheses</h3>
					<ol>
						<b><li class="fragment" data-fragment-index="1">Common data imbalance problem</li></b>
						<ul class="fragment" data-fragment-index="1">
							<li>Most images are close-up pictures of pets</li>
							<li>The network does not generalize to rare small examples</li>
						</ul>
						<br>
						<b><li class="fragment" data-fragment-index="2">Output Conversion</li></b>
						<ul class="fragment" data-fragment-index="2">
							<li>
								Loss of information when using the proposed output conversion <i>(i.e. last prediction of last layer)</i>
							</li>
						</ul>
					</ol>
				</section>
			</section>

			<!-- <section>
				<section data-markdown>
					### Futher experiments
					- Use harder datasets (small examples, etc.)
					- Test other output conversion :
					- Mean of all predictions
					- Median value of all predictions
					- Test other neural coding
					- Temporal coding, rank order, etc.

					Paper extension ?
				</section>
			</section> -->

			<section>
				<section>
					<h1>Conclusion and Perspectives</h1>
				</section>
				<section data-markdown>
					### Conclusion
					- First work on Supervised DCSNN for complex vision tasks

					- Proof-of-Concept results show the applicability of recent supervised approaches

					- First step towards more complex vision tasks
				</section>
				<section data-markdown>
					### Perspectives
					- Use event-based input instead of static images

					- Leads to a fully spike-based object tracking solution
					- Multiple object localization
					- Other tasks (object detection, segmentation, etc.)
				</section>
			</section>

			<section>
				<h1>Thanks !</h1>
			</section>

			<section>
				<h5>References</h5>
				<ul>
					<li><small><b>[Davies2018]</b> : <i>Davies, Mike, et al. "Loihi: A neuromorphic manycore processor
								with on-chip learning." Ieee Micro 38.1 (2018): 82-99.</i></small></li>
					<li><small><b>[Maass1997]</b> : <i>W. Maass, “Networks of spiking neurons: the third generation of
								neural network
								models,” Neural networks, vol. 10, no. 9, pp. 1659–1671, 1997.</i></small></li>
					<li><small><b>[Kaiser2020]</b> : <i> Kaiser, et al. “Synaptic plasticity dynamics for
								deep continuous local learning (decolle),” Frontiers in Neuroscience, vol. 14, p. 424,
								2020.</i></small></li>
					<li><small><b>[Caporale2008]</b> : <i> N. Caporale and Y. Dan, "Spike timing-dependent plasticity: a Hebbian learning rule" Annu. Rev. Neurosci., vol. 31, pp. 25–46, 2008.</i></small></li>
					<li><small><b>[Cao2015]</b> : <i>Y. Cao, et al. “Spiking deep convolutional neural
								networks for energy-efficient object recognition,” International Journal
								of Computer Vision, vol. 113, no. 1, pp. 54–66, 2015.</i></small></li>
					<li><small><b>[Falez2019]</b> : <i>P. Falez, et al. "Multi-layered spiking neural network with target timestamp threshold adaptation and stdp," in 2019 International Joint Conference on Neural Networks (IJCNN). IEEE, 2019, pp. 1–8.</i></small></li>
				</ul>
			</section>

			<section>
				<h3>Implementation Details</h3>

				<table>
					<tbody>
						<tr>
							<th>Implementation</th>
							<td>GPU with PyTorch</td>
						</tr>
						<tr>
							<th>GPU</th>
							<td>NVIDIA 2080Ti</td>
						</tr>
						<tr>
							<th>Batch size</th>
							<td>$16$</td>
						</tr>
						<tr>
							<th>Optimizer</th>
							<td>AdaMax $\beta_1=0$ & $\beta_2=0.95$</td>
						</tr>
						<tr>
							<th>Loss</th>
							<td>Smooth L1</td>
						</tr>
						<tr>
							<th>Learning rate</th>
							<td>$10^{-9}$</td>
						</tr>
						<tr>
							<th>Data augmentation</th>
							<td>Horizontal flipping and random brightness</td>
						</tr>
					</tbody>
				</table>
			</section>
		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script src="plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,
			slideNumber: 'c/t',
			math: {
				mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				config: 'TeX-AMS_HTML-full',
				// pass other options into `MathJax.Hub.Config()`
				TeX: { Macros: { RR: "{\\bf R}" } }
			},
			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
		});
	</script>
</body>

</html>