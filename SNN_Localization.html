<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Deep Spiking Convolutional Neural Network for Single Object Localization Based On Deep Continuous Local Learning</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/white.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<!-- Titre -->
			<section data-background-color="#ae2573">
				<h5>Deep Spiking Convolutional Neural Network for Single Object Localization Based On Deep Continuous Local Learning</h5>
				<img src="img/logo_udl_blanc.png">
				<small>Sami BARCHID &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; JosÃ© MENNESSON &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Chaabane DJÃ‰RABA</small>
			</section>

			<section data-markdown>
				# Summary
				- Spiking Neural Networks (SNNs)
				- SNN for Pattern Recognition
				- Deep Continuous Local Learning (DECOLLE)
				- Methodology
				- Experiment
				- Discussion & Future Works
				- Conclusion
			</section>

			<section>
				<section data-markdown>
					# Spiking Neural Networks
				</section>

				<section>
					<h3>Definition</h3>

					<ul>
						<li>
							An SNN consists of <b>spiking neurons</b> that communicate asynchronously through discrete spatio-temporal events called <b>spikes</b>.
						</li>
						<li>
							Leaky Integrate and Fire (LIF) neuron model
						</li>
					</ul>
					
					<img src="img/The-illustration-of-Leaky-Integrate-and-Fire-LIF-neuron-dynamics-The-pre-spikes-are.png">
					<small>
						Lee et al., "Enabling Spike-Based Backpropagation for Training Deep Neural Network Architectures"
					</small>
				</section>

				<!-- <section>
					<h3>Comparison with Artificial Neural Networks (ANN)</h3>

					<table>
						<thead>
							<th>ANN</th>
							<th>SNN</th>
						</thead>
						<tbody>
							<td></td>
						</tbody>
					</table>
				</section> -->

				<section data-markdown>
					### Simulation
					- SNNs can be simulated on GPU/CPU hardware
					- Discretize the continuous dynamics of spiking neurons into `$T$` successive timesteps
				</section>

				<section data-markdown>
					### Pros ðŸŸ¢
					- Can be implemented on low-power hardware
					- Unsupervised biological learning possible
				</section>

				<section data-markdown>
					### Cons ðŸ”´
					- Spiking neurons are not differentiable
						- No backpropagation
					- Performance still behind its ANN counterpart 
				</section>
			</section>

			<section>
				<section>
					<h1>SNN for Pattern Recognition</h1>
				</section>

				<section data-markdown data-auto-animate>
					### Learning Approaches
					1. ANN-to-SNN Conversion
					2. Backpropagation adaptation
					3. Unsupervised local learning
				</section>

				<section data-markdown data-auto-animate>
					### Learning Approaches
					1. ANN-to-SNN Conversion
					2. **Backpropagation adaptation**
					3. Unsupervised local learning
				</section>

				<section data-markdown>
					### State of SNN in Computer Vision
					- Simple recognition tasks *(e.g. digit recognition)*
					- Mostly shallow networks
					- Lack of works towards **complex vision tasks**
				</section>

				<section>
					<h3>Objective</h3>
					<p>Exploit recent works on supervised learning to perform complex tasks</p>
				</section>

			</section>

			<section>
				<section data-markdown>
					# Deep Continuous Local Learning
				</section>

				<section data-markdown>
					### Overview
					- Supervised surrogate gradient learning
					- Online learning
						- Synaptic weights are updated at each time-step
					-  Local error optimization
						- Implementable on neuromorphic computers
					- Low memory complexity
						- Enable deep network simulation
				</section>

				<section>
					<img src="img/DECOLLE.png">
					<i><small>Kaiser et al., "Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE)"</small></i>
				</section>
			</section>

			<section>
				<section data-markdown>
					# Methodology
				</section>

				<section>
					<h3>Overview</h3>
					<img src="img/FPN_DECOLLE_Overview.png">
				</section>

				<section data-markdown>
					### Image neural Coding

					- **Definition :** converts the numerical values of pixels into spikes.
					- No established method
					- Rate coding is used
				</section>

				<section>
					<img src="img/frequency_coding.png">
					<small>Falez et al., "Improving Spiking Neural Networks Trained with Spike Timing Dependent Plasticity for Image Recognition"</small>
				</section>

				<section data-markdown>
					### Readout Layers

					- A readout layer is attached to each layer
					- It produces a bounding box prediction
					- A DCSNN `$s(\mathbf{I})$` composed of `$L$` layers outputs `$L$` bounding box predictions.
					
					In short, &nbsp;&nbsp;&nbsp; `$s(\mathbf{I}) = \{\mathbf{B}_i\}^{L}_1$`
				</section>

				<section>
					<h3>Architecture</h3>
					<img src="img/architecture.png">
				</section>

				<section data-markdown>
					### Output Conversion Strategy
					- DECOLLE = online learning
						- An output prediction is produced for each of the `$T$` timesteps
						- One prediction for each readout layer
					- **Which prediction should we choose ?**
				</section>

				<section data-markdown>
					### Output Conversion Strategy
					- A rate-coded image delivers its total information after the `$T$` timesteps
						- **Keep the prediction of the last timestep**

					- Hierarchical representation of successive layers
						- **Keep the prediction of the last layer, `$\mathbf{B}_L$`**
				</section>
			</section>

			<section>
				<section>
					<h1>Experimental Proof-of-Concept</h1>
				</section>
				<section>
					<h3>Oxford-IIIT-Pet dataset</h3>
					<ul>
						<li>$176 \times 240$ dimension</li>
						<li>Training split: <b>6000 images</b></li>
						<li>Testing split: <b>1349 images</b></li>
					</ul>
					<img src="img/Oxford_IIIT_Pet.png">			
				</section>
				<section>
					<h3>Implementation Details</h3>
	
					<table>
						<tbody>
							<tr>
								<th>Implementation</th>
								<td>GPU with PyTorch</td>
							</tr>
							<tr>
								<th>GPU</th>
								<td>NVIDIA 2080Ti</td>
							</tr>
							<tr>
								<th>Batch size</th>
								<td>$16$</td>
							</tr>
							<tr>
								<th>Optimizer</th>
								<td>AdaMax $\beta_1=0$ & $\beta_2=0.95$</td>
							</tr>
							<tr>
								<th>Loss</th>
								<td>Smooth L1</td>
							</tr>
							<tr>
								<th>Learning rate</th>
								<td>$10^{-9}$</td>
							</tr>
							<tr>
								<th>Data augmentation</th>
								<td>Horizontal flipping and random brightness</td>
							</tr>
						</tbody>
					</table>
				</section>
				<section>
					<h3>Results</h3>
					<p>$63.2$% mIoU</p>

					<img src="img/qualitative_results.png">
				</section>

				<section data-markdown>
					### Observations
					- Most object are well-localized ($55$ to $75$% mIoU)
					- Small objects are poorly localized ($\le 15$% mIoU) 
				</section>

				<section data-markdown>
					### Hypotheses
					1. **Common data imbalance problem**
						- Most images are close-up pictures of pets
						- The network does not generalize to rare small examples
					2. **Output Conversion**
						- Loss of information when using the proposed output conversion *(i.e. last prediction of last layer)*
				</section>
			</section>

			<section>
				<section>
					<h1>Future Works</h1>
				</section>

				<section data-markdown>
					### Further experiments
					- Use harder datasets (small examples, etc)
					- Test other output conversion :
						- Mean of all predictions
						- Median value of all predictions
					- Test other neural coding
						- Temporal coding, rank order, etc
				</section>
				<section data-markdown>
					### Other ideas
					- Use event-based input instead of static images
						- Lead to a fully spike-based object tracking solution
					- Multiple object localization
					- Other tasks (object detection, segmentation, etc)
				</section>

				<section data-markdown>
					### Collaboration
					- If you want to use DECOLLE for your tasks, tell me ! 
				</section>
			</section>
		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script src="plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,
			slideNumber: 'c/t',
			math: {
				mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				config: 'TeX-AMS_HTML-full',
				// pass other options into `MathJax.Hub.Config()`
				TeX: { Macros: { RR: "{\\bf R}" } }
			},
			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
		});
	</script>
</body>

</html>