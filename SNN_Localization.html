<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Deep Spiking Convolutional Neural Network for Single Object Localization Based On Deep Continuous Local
		Learning</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/white.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<!-- Titre -->
			<section>
				<h5 style="text-transform: capitalize;">Deep Spiking Convolutional Neural Network for Single Object
					Localization Based On Deep Continuous Local Learning</h5>
				<br>
				<br>
				<small><b>Sami BARCHID</b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; José MENNESSON &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					Chaabane DJÉRABA</small>
				<img src="img/logos_cbmi.png">
				<p><small id="js-current-date"></small></p>
				<script>
					document.getElementById('js-current-date').innerHTML = new Date().toLocaleDateString("en-US")
				</script>
			</section>

			<section>
				<section data-markdown>
					# Context
				</section>
				<section>
					<h3>Deep learning</h3>
					<ul>
						<li>State-of-the-art in almost all Computer Vision and Machine Learning tasks</li>
						<li>Growing computational, memory and energy costs</li>
					</ul>
					<img src="img/carbon-footprint.png">
				</section>

				<section>
					<h3>Spiking Neural Networks <span style="text-transform:none; font-weight: normal; font-style: italic;">(SNNs)</span> </h3>
					<ul>
						<li>Third-generation of AI <small>[Maass1997]</small></li>
						<li>Spiking neurons : strongly inspired by the neurons of our brain</li>
						<li>Implementable on low-power neuromorphic hardware <small>[Davies2018]</small></li>
						<li><b>A potential solution</b></li>
					</ul>
				</section>

				<section>
					<h3>Challenges</h3>
					<ul>
						<li>Spiking neurons are not differentiable <small>[Kaiser2020]</small></li>
						<ul>
							<li><b>No backpropagation</b></li>
						</ul>
						<li>Performance still behind Artificial Neural Networks (ANNs)</li>
					</ul>
				</section>
			</section>

			<section>
				<section data-markdown>
					# SNN for Pattern Recognition
				</section>

				<section>
					<h3>Definition</h3>

					<ul>
						<li>
							An SNN consists of <b>spiking neurons</b> that communicate asynchronously through discrete
							spatio-temporal events called <b>spikes</b>.
						</li>
						<li>
							Leaky Integrate and Fire (LIF) neuron model
						</li>
					</ul>

					<img
						src="img/The-illustration-of-Leaky-Integrate-and-Fire-LIF-neuron-dynamics-The-pre-spikes-are.png">
					<small>
						Lee, Chankyu, et al. "Enabling spike-based backpropagation for training deep neural network architectures." Frontiers in neuroscience 14 (2020).
					</small>
				</section>

				<section data-markdown data-auto-animate>
					### Learning Approaches
					1. ANN-to-SNN Conversion
					2. Backpropagation adaptation
					3. Unsupervised local learning
				</section>

				<section data-markdown data-auto-animate>
					### Learning Approaches
					1. ANN-to-SNN Conversion
					2. **Backpropagation adaptation**
					3. Unsupervised local learning
				</section>

				<section data-markdown>
					### State of SNN in Computer Vision
					- Simple recognition tasks *(e.g. digit recognition)*
					- Mostly shallow networks
					- Lack of works towards **complex vision tasks**
				</section>

				<section>
					<h3>Objective</h3>
					<p>Exploit recent works on supervised learning to perform complex tasks</p>
				</section>

			</section>

			<section>
				<section data-markdown>
					# Deep Continuous Local Learning
				</section>

				<section data-markdown>
					### Overview
					- Supervised surrogate gradient learning
					- Online learning
					- Synaptic weights are updated at each time-step
					- Local error optimization
					- Implementable on neuromorphic computers
					- Low memory complexity
					- Enable deep network simulation
				</section>

				<section>
					<img src="img/DECOLLE.png">
					<i><small>Kaiser et al., "Synaptic Plasticity Dynamics for Deep Continuous Local Learning
							(DECOLLE)"</small></i>
				</section>
			</section>

			<section>
				<section data-markdown>
					# Methodology
				</section>

				<section>
					<h3>Overview</h3>
					<img src="img/FPN_DECOLLE_Overview.png">
				</section>

				<section data-markdown>
					### Image neural Coding

					- **Definition :** converts the numerical values of pixels into spikes.
					- No established method
					- Rate coding is used
				</section>

				<section>
					<img src="img/frequency_coding.png">
					<small>Falez et al., "Improving Spiking Neural Networks Trained with Spike Timing Dependent
						Plasticity for Image Recognition"</small>
				</section>

				<section data-markdown>
					### Readout Layers

					- A readout layer is attached to each layer
					- It produces a bounding box prediction
					- A DCSNN `$s(\mathbf{I})$` composed of `$L$` layers outputs `$L$` bounding box predictions.

					In short, &nbsp;&nbsp;&nbsp; `$s(\mathbf{I}) = \{\mathbf{B}_i\}^{L}_1$`
				</section>

				<section>
					<h3>Architecture</h3>
					<img src="img/architecture.png">
				</section>

				<section data-markdown>
					### Output Conversion Strategy
					- DECOLLE = online learning
					- An output prediction is produced for each of the `$T$` timesteps
					- One prediction for each readout layer
					- **Which prediction should we choose ?**
				</section>

				<section data-markdown>
					### Output Conversion Strategy
					- A rate-coded image delivers its total information after the `$T$` timesteps
					- **Keep the prediction of the last timestep**

					- Hierarchical representation of successive layers
					- **Keep the prediction of the last layer, `$\mathbf{B}_L$`**
				</section>
			</section>

			<section>
				<section>
					<h1>Experimental Proof-of-Concept</h1>
				</section>
				<section>
					<h3>Oxford-IIIT-Pet dataset</h3>
					<ul>
						<li>$176 \times 240$ dimension</li>
						<li>Training split: <b>6000 images</b></li>
						<li>Testing split: <b>1349 images</b></li>
					</ul>
					<img src="img/Oxford_IIIT_Pet.png">
				</section>
				<section>
					<h3>Implementation Details</h3>

					<table>
						<tbody>
							<tr>
								<th>Implementation</th>
								<td>GPU with PyTorch</td>
							</tr>
							<tr>
								<th>GPU</th>
								<td>NVIDIA 2080Ti</td>
							</tr>
							<tr>
								<th>Batch size</th>
								<td>$16$</td>
							</tr>
							<tr>
								<th>Optimizer</th>
								<td>AdaMax $\beta_1=0$ & $\beta_2=0.95$</td>
							</tr>
							<tr>
								<th>Loss</th>
								<td>Smooth L1</td>
							</tr>
							<tr>
								<th>Learning rate</th>
								<td>$10^{-9}$</td>
							</tr>
							<tr>
								<th>Data augmentation</th>
								<td>Horizontal flipping and random brightness</td>
							</tr>
						</tbody>
					</table>
				</section>
				<section>
					<h3>Results</h3>
					<p>$63.2$% mIoU</p>

					<img src="img/qualitative_results.png">
				</section>

				<section data-markdown>
					### Observations
					- Most object are well-localized ($55$ to $75$% mIoU)
					- Small objects are poorly localized ($\le 15$% mIoU)
				</section>

				<section data-markdown>
					### Hypotheses
					1. **Common data imbalance problem**
					- Most images are close-up pictures of pets
					- The network does not generalize to rare small examples
					2. **Output Conversion**
					- Loss of information when using the proposed output conversion *(i.e. last prediction of last
					layer)*
				</section>
			</section>

			<section>
				<section>
					<h1>Future Works</h1>
				</section>

				<section data-markdown>
					### Futher experiments
					- Use harder datasets (small examples, etc)
					- Test other output conversion :
					- Mean of all predictions
					- Median value of all predictions
					- Test other neural coding
					- Temporal coding, rank order, etc

					Paper extension ?
				</section>
				<section data-markdown>
					### Other ideas
					- Use event-based input instead of static images
					- Lead to a fully spike-based object tracking solution
					- Multiple object localization
					- Other tasks (object detection, segmentation, etc)
				</section>

				<section>
					<h3>Collaboration</h3>
					<p>If you want to use DECOLLE for your tasks, tell me ! </p>
				</section>
			</section>

			<section>
				<section data-markdown>
					# Conclusion
					- First work on Supervised DCSNN for complex vision tasks
					- Proof-of-Concept results show the applicability of recent supervised approaches
					- First step towards harder vision tasks *(e.g. object detection, semantic segmentation)*
				</section>
			</section>
			<section data-markdown>
				### References
				- **[Davies2018]**: *Davies, Mike, et al. "Loihi: A neuromorphic manycore processor with on-chip
				learning." Ieee Micro 38.1 (2018): 82-99.*
				- **[Maass1997]**: *W. Maass, “Networks of spiking neurons: the third generation of neural network
				models,” Neural networks, vol. 10, no. 9, pp. 1659–1671, 1997.*
				- **[Kaiser2020]**: *J. Kaiser, et al. “Synaptic plasticity dynamics for
				deep continuous local learning (decolle),” Frontiers in Neuroscience, vol. 14, p. 424, 2020.*
			</section>
		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script src="plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,
			slideNumber: 'c/t',
			math: {
				mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				config: 'TeX-AMS_HTML-full',
				// pass other options into `MathJax.Hub.Config()`
				TeX: { Macros: { RR: "{\\bf R}" } }
			},
			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
		});
	</script>
</body>

</html>