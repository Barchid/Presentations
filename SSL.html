<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Self-supervised Learning for Event-based Vision </title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/white.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
	<link rel="stylesheet" href="css/w3.css">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<!-- Titre -->
			<section>
				<h3 style="text-transform: none;">Self-supervised Learning for Event-based Vision</h3>
				<br>
				<!-- <img src="img/Bina_rep/icip22.png" alt="ICIP22"> -->
				<!-- <br> -->
				<p class="fragment" style="font-style: italic;">
					Laying the foundation of Self-Supervised Learning (SSL) applied to event-cameras
				</p>

				<br>
				<small><b>Sami BARCHID</b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Jos√© MENNESSON &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					Chaabane DJ√âRABA</small>
				<img src="img/Bina_rep/logos_cbmi.png">
				<!-- <p><small id="js-current-date"></small></p>
				<script>
					document.getElementById('js-current-date').innerHTML = new Date().toLocaleDateString("en-US")
				</script> -->
			</section>

			<section>
				<section>
					<h1>Introduction</h1>
					<!-- <ol class="fragment">
						<li>Event-based Vision</li>
						<br>
						<li>Self-Supervised Learning</li>
					</ol> -->
				</section>
				<section>
					<h3>Event Cameras</h3>
					<!-- <video src="img/Bina_rep/dvscamvisu.mp4" height="300px" muted autoplay></video> -->
					<video src="img/Bina_rep/dvscamvisu.mp4" height="300px" muted autoplay></video>
					<p><small>From video here: <a
								href="https://www.youtube.com/watch?v=LauQ6LWTkxM">https://www.youtube.com/watch?v=LauQ6LWTkxM</a></small>
					</p>
				</section>

				<!-- <section>
					<h3>Advantages</h3>
					<p id="js-pros-choice" style="font-weight: bold;"></p>
					<p><button id="js-pros" class="w3-button w3-block w3-blue">Play</button></p>

					<script>
						function get_random (list) {
							return list[Math.floor((Math.random()*list.length))];
						}
						names = ["Jos√©", "Chaabane", "Marius", "Pierre", "Gaspard", "Akrem", "Mireille", "Matthieu"]

						prosBtn = document.getElementById('js-pros')
						prosText = document.getElementById('js-pros-choice')
						timeid = undefined

						prosBtn.onclick = () => {
							if(timeid){
								return
							}

							timeid = setInterval(() => {
								prosText.innerHTML = get_random(names)
							}, 75)

							setTimeout(() => {
								clearInterval(timeid)
								timeid = undefined
							}, 1500)
						}

					</script>
				</section> -->

				<section>
					<h3>Advantages</h3>
					<ul>
						<li>Sparser representation</li>
						<br>
						<li>üöÄ Low latency (~1 ¬µs)</li>
						<br>
						<li style="text-decoration: line-through;">Motion blur</li>
						<br>
						<li> üõ°Ô∏è Higher dynamic range</li>
						<br>
						<li> üåç <b>Energy efficiency</b></li>
					</ul>
				</section>

				<section>
					<h3>Event-based Vision</h3>
					<p><b style="color: red">Conventional vision algorithms cannot be applied directly.</b></p>
					<ol>
						<li class="fragment"><b>Asynchronous events</b> <span style="font-style: italic">instead
								of</span> <b>frames</b></li>
						<br>
						<li class="fragment"><b>Binary changes</b> <span style="font-style: italic">instead of</span>
							<b>intensity values</b>
						</li>
					</ol>
				</section>

				<section data-transition="none">
					<h3>Event representation</h3>
					<img src="img/Bina_rep/event_representation_0.png" alt="Event representation">
					<p><b>Definition:</b> method that takes asynchronous events as input, and transforms them into an
						alternative representation (e.g. binary event frames).</p>
				</section>
				<section data-transition="none">
					<h3>Event representation</h3>
					<img src="img/Bina_rep/event_representation_1.png" alt="Event representation">
					<p><b>Definition:</b> method that takes <span style="color: red;">asynchronous events as
							input</span>, and transforms them into an
						alternative representation (e.g. binary event frames).</p>
				</section>

				<section data-transition="none">
					<h3>Event representation</h3>
					<img src="img/Bina_rep/event_representation.png" alt="Event representation">
					<p><b>Definition:</b> method that takes asynchronous events as input, and transforms them into an
						<span style="color: red;">alternative representation</span> (e.g. binary event frames).
					</p>
				</section>

				<section>
					<h3>Popular Strategies</h3>
					<ol>
						<li>Event Frames + conventional algorithm</li>
						<br>
						<li>Convolutional Spiking Neural Networks</li>
					</ol>
				</section>

				<section>
					<h3>Self-Supervised Learning</h3>
					<ul>
						<li><b>üîé : </b> learn useful representation of the data <b>without supervision</b></li>
						<br>
						<li><b>Pre-training</b> (SSL) ‚û°Ô∏è <b>downstream task</b> (fine-tuning)</li>
						<br>
						<li class="fragment" data-fragment-index="1" style="color: green;"><b>Pros</b></li>
						<ul class="fragment" data-fragment-index="1" style="color: green;">
							<li>Data efficiency</li>
							<li>Great progress in frame-based vision recently <span
									style="font-style: italic;">(Contrastive Learning, ...)</span></li>
						</ul>
					</ul>
				</section>

				<section>
					<h3>Intuition</h3>
					<p><b style="color: red;">SSL is not explored in event-based vision</b></p>
					<ul class="fragment">
						<li>No baseline</li>
						<li>No benchmark</li>
						<li>No investigation</li>
						<li>...</li>
					</ul>
					<br>
					<p class="fragment" style="color: green"><b>üéØ Foundational study of event-based SSL üéØ</b></p>
					<!-- <ul>
						<li class="fragment" data-fragment-index="0">Public event-based datasets are often small</li>
						<br>
						<li class="fragment" data-fragment-index="1"><b>SSL for frame-based vision</b></li>
						<ul class="fragment" data-fragment-index="1">
							<li class="fragment" data-fragment-index="2">Unsupervised Representation Learning</li>
							<li class="fragment" data-fragment-index="3">Data-efficient feature extraction</li>
							<li class="fragment" data-fragment-index="4">Benefits to frame-based vision algorithms</li>
						</ul>
						<br>
						<li class="fragment" data-fragment-index="5">Event-based SSL can offer the same advantage in event-based vision</li>
						<ul class="fragment" data-fragment-index="5">
							<li> <b>Solid context to study fundamental techniques</b> (e.g. data augmentation)</li>
						</ul>
					</ul> -->
				</section>

				<section>
					<h3>Contributions</h3>
					<ol>
						<li>First evaluation protocols</li>
						<br>
						<li>First baseline methods</li>
						<br>
						<li>Impact of event-based distortions on contrastive learning efficiency</li>
						<br>
						<ul class="fragment">
							<li>New data augmentation methods to supplement the existing approaches</li>
						</ul>
					</ol>
				</section>
			</section>

			<section>
				<section>
					<h1>Theory</h1>
				</section>
			</section>

			<section>
				<section>
					<h1>Related Works</h1>
				</section>

				<section>
					<h3></h3>
				</section>
			</section>

			<section>
				<section>
					<h1>Proposed Method</h1>
				</section>

				<section>
					<h3>Formulation</h3>
					<p><b>Spike Tensor</b></p>
					<div class="r-stack">
						<img alt="spike_tensor" src="img/ssl/spike_tensor.drawio_0.png">
						<img class="fragment" alt="spike_tensor" src="img/ssl/spike_tensor.drawio_1.png">
						<img class="fragment" alt="spike_tensor" src="img/ssl/spike_tensor.drawio_2.png">
						<img class="fragment" alt="spike_tensor" src="img/ssl/spike_tensor.drawio_3.png">
						<img class="fragment" alt="spike_tensor" src="img/ssl/spike_tensor.drawio_4.png">
						<img class="fragment" alt="spike_tensor" src="img/ssl/spike_tensor.drawio_5.png">
						<img class="fragment" alt="spike_tensor" src="img/ssl/spike_tensor.drawio_6.png">
						<img class="fragment" alt="spike_tensor" src="img/ssl/spike_tensor.drawio.png">
						<img class="fragment" alt="spike_tensor" src="img/ssl/spike_tensor.drawio_8.png">
						<img src="img/Bina_rep/hand_clap.gif" alt="clap" class="fragment">
					</div>
				</section>

				<section>
					<h3>Formulation</h3>
					<p><b>Neuromorphic Data Augmentation</b> (NDA)</p>
					<div class="r-stack">
						<img src="img/ssl/dataaug_0.png" data-fragment-index="0" alt="data" class="fragment">
						<img src="img/ssl/dataaug_1.png" data-fragment-index="1" alt="data" class="fragment">
						<img src="img/ssl/dataaug.png" data-fragment-index="2" alt="data" class="fragment">
						<img src="img/ssl/dataaug_comp.png" alt="data" class="fragment" data-fragment-index="3">
						<img src="img/ssl/dataaug_comp2.png" alt="data" class="fragment" data-fragment-index="4">
						<img src="img/ssl/dataaug_comp3.png" alt="data" class="fragment" data-fragment-index="5">
					</div>
					<p class="fragment" data-fragment-index="3">NDAs can be <b>compositions</b> of NDAs</p>
				</section>

				<section>
					<h3>Method</h3>
					<p><b>Overview</b></p>
					<div class="r-stack">
						<img alt="model" class="fragment" src="img/ssl/model_0.png">
						<img alt="model" class="fragment" src="img/ssl/model_1.png">
						<img alt="model" class="fragment" src="img/ssl/model_2.png">
						<img alt="model" class="fragment" src="img/ssl/model_3.png">
						<img alt="model" class="fragment" src="img/ssl/model_4.png">
						<img alt="model" class="fragment" src="img/ssl/model_5.png">
						<img alt="model" class="fragment" src="img/ssl/model_6.png">
						<img alt="model" class="fragment" src="img/ssl/model.png">
					</div>
				</section>

				<section>
					<h3>Method</h3>
					<p><b>Encoder Architectures</b></p>
					<div class="r-stack">
						<div class="fragment fade-out">
							<p><b>Challenges:</b></p>
							<ul>
								<li>Adapt the spike tensor to fit to a specific type of conv. encoder</li>
								<br>
								<li>Representations $\mathbf{Y}^d \in \mathbb{R}^m$ do <b>not</b> have any temporal dimension</li>
								<br>
								<li>...</li>
							</ul>
						</div>
						<img src="img/ssl/encoder_cnn.png" alt="encoder" class="fragment fade-in-then-out">
						<img src="img/ssl/encoder_3dcnn.png" alt="encoder" class="fragment fade-in-then-out">
						<img src="img/ssl/encoder_snn.png" height="550px" alt="encoder" class="fragment fade-in-then-out">
					</div>
					
				</section>

				<section>
					<h3>Method</h3>
					<p><b>Projector</b></p>
					<img src="img/ssl/projector.png" alt="proj">
				</section>

				<section>
					<h3>Method</h3>
					<p><b>Variants</b></p>
					<div class="r-stack">
						<img class="fragment" data-fragment-index="0" src="img/ssl/variant_twin.png" alt="twin">
						<img class="fragment" data-fragment-index="1" src="img/ssl/variant_teach.png" alt="twin">
					</div>
					<div class="r-stack">
						<p class="fragment fade-in-then-out" data-fragment-index="0">Twins</p>
						<p class="fragment" data-fragment-index="1">Student - Teacher</p>
					</div>
				</section>
			</section>

			<section>
				<section>
					<h1>Neuromorphic Data Augmentation</h1>
				</section>

				<section>
					<img src="img/ssl/lift.gif" height="400px" alt="lift">
				</section>

				<section>
					<h3>Existing</h3>
					<p><b>Flip Polarity</b></p>
					<img src="img/ssl/lift_flipo.gif" height="400px" alt="lift">
				</section>

				<section>
					<h3>Previous works</h3>
					<p><b>Reverse</b></p>
					<img src="img/ssl/lift_reverse.gif" height="400px" alt="lift">
				</section>

				<section>
					<h3>Previous works</h3>
					<p><b>Random Resized Crop</b></p>
					<img src="img/ssl/lift_crop.gif" height="400px" alt="lift">
					<p>Scale $\in [0.08, 1.0]$</p>
				</section>

				<section>
					<h3>Previous works</h3>
					<p><b>Static Translation</b></p>
					<img src="img/ssl/lift_static_translation.gif" height="350px" alt="lift">
					<p>$l_x \in [- 0.2 \times W, 0.2 \times W]$</p>
					<p>$l_y \in [- 0.2 \times H, 0.2 \times H]$</p>
				</section>

				<section>
					<h3>Previous works</h3>
					<p><b>Static Rotation</b></p>
					<img src="img/ssl/lift_static_rotation.gif" height="400px" alt="lift">
					<p>Degrees $\in [-75¬∞,+75¬∞]$</p>
				</section>

				<section>
					<h3>Previous Works</h3>
					<p><b>Cutout</b></p>
					<img src="img/ssl/lift_cutout.gif" height="350px" alt="lift">
					<p>$N_{hole} \in [1,3]$ of scale $\in [0.3, 0.6]$ each</p>
				</section>

				<section>
					<h3>Previous Works</h3>
					<p><b>EventDrop</b></p>
					<div class="r-stack">
						<img src="img/ssl/eventdrop.png" data-fragment-index="0" alt="lift" class="fragment fade-in-then-out">
						<img src="img/ssl/lift_evdrop_area.gif" height="400px" alt="lift" class="fragment fade-in-then-out" data-fragment-index="1">
						<img src="img/ssl/lift_evdrop_time.gif" height="400px" alt="lift" class="fragment fade-in-then-out" data-fragment-index="2">
						<img src="img/ssl/lift_evdrop_drop.gif" height="400px" alt="lift" class="fragment fade-in" data-fragment-index="3">
					</div>
					<div class="r-stack">
						<p class="fragment fade-in-then-out" data-fragment-index="1">Drop by Area</p>
						<p class="fragment fade-in-then-out" data-fragment-index="2">Drop by Time</p>
						<p class="fragment fade-in" data-fragment-index="3">Drop random events</p>
					</div>
				</section>

				<section>
					<h3>Proposed</h3>
					<p><b>Background Activity Noise</b> (BA Noise)</p>
					<img src="img/ssl/lift_ba.gif" alt="lift" height="400px">
					
				</section>

				<section>
					<h3>Proposed</h3>
					<p><b>Dynamic Translation</b></p>
					<img src="img/ssl/lift_dyn_translation.gif" height="350px" alt="lift">
					<p>$l_x \in [- 0.2 \times W, 0.2 \times W]$</p>
					<p>$l_y \in [- 0.2 \times H, 0.2 \times H]$</p>
				</section>

				<section>
					<h3>Proposed</h3>
					<p><b>Dynamic Rotation</b></p>
					<img src="img/ssl/lift_dyn_rotation.gif" height="400px" alt="lift">
					<p>Degrees $\in [-75¬∞,+75¬∞]$</p>
				</section>
			</section>

			<section>
				<section>
					<h1>Evaluation Protocols</h1>
				</section>

				<section>
					<h3>Objectives</h3>
					<ul>
						<li>We want to evaluate a model on its...</li>
						<br>
						<ul>
							<li class="fragment">üéØ <b>accuracy</b> obtained using its representations</li>
							<br>
							<li class="fragment">üíæ <b>(data-)efficiency </b> to reduce the need of labeled data</li>
							<br>
							<li class="fragment">üîÅ <b>transferability</b> to other datasets</li>
							<br>
							<li class="fragment">üîÄ <b>transferability</b> to other tasks</li>
						</ul>
					</ul>
				</section>

				<section>
					<h3>Additional considerations</h3>
					<p style="color: orange">‚ö†Ô∏è Event-based datasets variability ‚ö†Ô∏è</p>
					<div class="r-stack">
						<img class="fragment fade-out" data-fragment-index="0" src="img/ssl/lift.gif" alt="Lift"
							height="400px">
						<img class="fragment current-visible" data-fragment-index="0" src="img/ssl/ncaltech101.gif"
							height="400px" alt="calt">
					</div>
					<div class="r-stack">
						<p class="fragment fade-out" data-fragment-index="0">Still Camera + moving objects</p>
						<p class="fragment current-visible" data-fragment-index="0">Moving camera + moving/still objects
						</p>
					</div>

					<!-- TODO -->
				</section>

				<section>
					<h3>1. Linear Evaluation</h3>
					<div class="r-stack">
						<img alt="eval" src="img/ssl/ssl_conv_enc.png" class="fragment fade-out"
							data-fragment-index="0">
						<img src="img/ssl/ssl_conv_enc_ssl.png" alt="alt" class="fragment current-visible"
							data-fragment-index="0">
						<img src="img/ssl/ssl_conv_enc_linear.png" alt="lin" class="fragment">
						<img src="img/ssl/ssl_conv_enc_linear_train.png" alt="lin" class="fragment">
						<img src="img/ssl/ssl_conv_enc_linear_variant.png" alt="lin" class="fragment">
					</div>
					<p class="fragment">
						<b>Metric:</b> top-1 accuracy on linear classifier
					</p>
				</section>

				<section>
					<h3>2. Semi-supervised Training</h3>
					<div class="r-stack">
						<img alt="eval" src="img/ssl/ssl_conv_enc.png" class="fragment fade-out"
							data-fragment-index="0">
						<img src="img/ssl/ssl_conv_enc_ssl.png" alt="alt" class="fragment current-visible"
							data-fragment-index="0">
						<img src="img/ssl/ssl_conv_enc_linear2.png" alt="lin" class="fragment">
						<img src="img/ssl/ssl_conv_enc_finetune.png" alt="lin" class="fragment">
						<img src="img/ssl/ssl_conv_enc_finetune2.png" alt="lin" class="fragment">
					</div>

					<div class="fragment">
						<p><b>Metrics:</b> top-1 accuracy on</p>
						<ul>
							<li>10% of the train set</li>
							<li>25% of the train set</li>
						</ul>
					</div>
				</section>

				<section>
					<h3>3. Transfer to other datasets</h3>
					<div class="r-stack">
						<img alt="eval" src="img/ssl/ssl_conv_enc.png" class="fragment fade-in-then-out">
						<img src="img/ssl/ssl_conv_enc_ssl2_0.png" alt="alt" class="fragment fade-in-then-out">
						<img src="img/ssl/ssl_conv_enc_ssl2.png" alt="alt" class="fragment fade-in-then-out">
						<img src="img/ssl/ssl_conv_enc_freeze.png" alt="lin" class="fragment">
						<img src="img/ssl/ssl_conv_enc_linear.png" alt="lin" class="fragment">
						<img src="img/ssl/ssl_conv_enc_linear_train.png" alt="in" class="fragment">
						<img src="img/ssl/ssl_conv_enc_linear_variant2.png" alt="in" class="fragment">
					</div>
					<div class="fragment">
						<p><b>Metrics:</b> top-1 accuracy</p>
						<ul>
							<li>1Ô∏è‚É£ : <b>DVSGesture</b> <span style="font-style: italic;">(SSL)</span> ‚û°Ô∏è <b>DailyAction-DVS</b> <span style="font-style: italic;">(fine.)</span></li>
							<li>2Ô∏è‚É£ : <b>N-Caltech101</b> <span style="font-style: italic;">(SSL)</span> ‚û°Ô∏è <b>N-MNIST</b> <span style="font-style: italic;">(fine.)</span></li>
						</ul>
					</div>
				</section>

				<section>
					<h3>4. Transfer to other tasks</h3>
					<p><b>Object Detection</b></p>
					<div class="r-stack">
						<img alt="eval" src="img/ssl/ssl_conv_enc.png" class="fragment fade-out"
							data-fragment-index="0">
						<img src="img/ssl/ssl_conv_enc_ssl.png" alt="alt" class="fragment current-visible"
							data-fragment-index="0">
						<img src="img/ssl/ssl_conv_enc_det.png" alt="lin" class="fragment">
						<img src="img/ssl/ssl_conv_enc_det_1.png" alt="lin" class="fragment">
						<img src="img/ssl/ssl_conv_enc_det_2.png" alt="lin" class="fragment">
					</div>
					<p class="fragment">
						50% of <b>Gen1</b> <span style="font-style: italic;">(SSL)</span> ‚û°Ô∏è 50% of <b>Gen1</b> <span style="font-style: italic;">(fine.)</span>
					</p>
				</section>
			</section>

			<section>
				<section>
					<h1>Experiments</h1>
				</section>

				<section>
					<h3>Datasets and metrics</h3>
					<img src="img/Bina_rep/table1.png" alt="Table 1">
					<ul>
						<li>Metric used: top-1 accuracy</li>
					</ul>
				</section>

				<section>
					<h3>Implementation Details</h3>
					<ul>
						<li>PyTorch 1.8 on one NVIDIA Tesla P100 GPU</li>
						<li>Resolution: $128 \times 128$</li>
						<li>Employed architectures:</li>
						<ul>
							<li>CNN: <b>ResNet-18</b><small>[Kaiming2016]</small></li>
							<li>3DCNN: <b>MC3-ResNet-18</b><small>[Kaiming2016]</small></li>
							<li>SNN: <b>SEW-ResNet-18</b><small>[Kaiming2016]</small></li>
						</ul>
						<li>Trained during <b>500 epochs</b> using <b>Adam optimizer</b> with a <b>learning rate of
								$0.001$</b></li>
						<li class="fragment highlight-red">8-bit <code>uint</code> representation used for Bina-Rep
							(i.e. $N=8$)</li>
					</ul>
				</section>

				<section>
					<h3>Comparison with Event Representations (1)</h3>
					<div class="r-stack">
						<img src="img/Bina_rep/table2.png" alt="Table 2">
						<img class="fragment" src="img/Bina_rep/table2_1.png" alt="Table 2">
					</div>
				</section>

				<section>
					<h3>Comparison with Event Representations (2)</h3>
					<div class="r-stack">
						<img src="img/Bina_rep/table3.png" alt="Table 3">
						<img src="img/Bina_rep/table3_1.png" alt="table3" class="fragment">
						<img src="img/Bina_rep/table3_2.png" alt="table3" class="fragment">
						<img src="img/Bina_rep/table3_3.png" alt="table3" class="fragment">
						<img src="img/Bina_rep/table3_4.png" alt="table3" class="fragment">
						<img src="img/Bina_rep/table3_5.png" alt="table3" class="fragment">
						<img src="img/Bina_rep/table3_6.png" alt="table3" class="fragment">
						<img src="img/Bina_rep/table3_7.png" alt="table3" class="fragment">
					</div>
				</section>

				<section>
					<h3>Comparison with SOTA</h3>
					<div class="r-stack">
						<img src="img/Bina_rep/table4.png" alt="Table 4">
						<!-- <img src="img/Bina_rep/table4_1.png" class="fragment" alt="Table 4">
						<img src="img/Bina_rep/table4_2.png" class="fragment" alt="Table 4"> -->
					</div>
				</section>

				<section>
					<h3>Robustness Analysis</h3>
					<h4>Corruptions</h4>
					<div class="r-stack">
						<img src="img/Bina_rep/example_corruptions_0.png" alt="Corruptions example">
						<img class="fragment" src="img/Bina_rep/example_corruptions_1.png" alt="Corruptions example">
						<img class="fragment" src="img/Bina_rep/example_corruptions.png" alt="Corruptions example">
					</div>
					<p><b>Dataset used</b>: N-Cars<small>[Sironi2018]</small></p>
				</section>

				<section>
					<h3>Robustness Analysis</h3>
					<h4>severity levels</h4>
					<p>For a specific corruption: performance are evaluated on increasing <b>severity levels</b> (From
						$1$ to $5$)</p>
					<ul>
						<li><b>Background Activity</b>: adds a percentage of noisy events</li>
						<li><b>Occlusion</b>: increases the occlusion area (percentage of original resolution)</li>
					</ul>
					<img src="img/Bina_rep/table5.png" alt="Table 5">
				</section>

				<section>
					<h3>Robustness Analysis</h3>
					<h4>Metric</h4>
					<p>
						<b>Relative Acurracy Drop</b> for a given severity level $i$:
					</p>
					<br>
					<p> $score = \frac{acc_0-acc_i}{acc_0} \times
						100$</p>
					<br>
					<ul>
						<li>$acc_0$ = the accuracy score on clean data</li>
						<li>$acc_i$ = the accuracy score on data corrupted by severity level $i$</li>
					</ul>
				</section>

				<section>
					<img src="img/Bina_rep/fig3.png" alt="Fig 3">
					<ul>
						<li class="fragment">In general, the same evolution is observed for all methods</li>
						<li class="fragment">Bina-Rep is on par with SOTA event representation methods</li>
						<li class="fragment">Bina-Rep becomes more robust with more frames ($T \gt 1$)</li>
					</ul>
				</section>
			</section>

			<section>
				<section>
					<h1>Conclusion</h1>
				</section>
				<section>
					<h3>Discussion</h3>
					<ol>
						<li class="fragment" data-fragment-index="1">Bina-Rep event frames express 3 information</li>
						<ul class="fragment">
							<li>The <b>presence/absence</b> of an event</li>
							<li>The <b>number of events</b> that occured</li>
							<li>The <b>order of these events</b> in the sequence</li>
						</ul>
						<br>
						<li class="fragment">Better or competitive performance against SOTA
						</li>
						<br>
						<li class="fragment">Competitive robustness scores against common corruptions of event cameras.
						</li>
					</ol>
				</section>

				<section>
					<h3>Try it yourself</h3>
					<p>Available in the <a href="https://github.com/neuromorphs/tonic">Tonic Library</a></p>
					<img height="100px" src="img/Bina_rep/tonic.png" alt="tonic logo">
					<br>
					<img class="fragment" src="img/Bina_rep/tobinarep.png" alt="code example">
				</section>
			</section>
			<section>
				<section>
					<p class="r-fit-text">Thank you!</p>
				</section>
				<section>
					<h3>References</h3>
					<ul>
						<li><small><b>[Kogler2009]: </b>Kogler, et al. "Bio-inspired stereo vision system with silicon
								retina imagers." International Conference on Computer Vision Systems. Springer, Berlin,
								Heidelberg, 2009</small></li>
						<br>
						<li><small><b>[Kaiming2016]: </b> He, Kaiming, et al. "Deep residual learning for image
								recognition." Proceedings of the IEEE conference on computer vision and pattern
								recognition. 2016.</small></li><br>
						<li><small><b>[Sironi2018]: </b>Sironi, et al. "HATS: Histograms of averaged time surfaces for
								robust event-based object classification." Proceedings of the IEEE Conference on
								Computer Vision and Pattern Recognition. 2018.</small></li><br>
						<li><small><b>[Maqueda2018]: </b>Maqueda, et al. "Event-based vision meets deep learning on
								steering prediction for self-driving cars." Proceedings of the IEEE conference on
								computer vision and pattern recognition. 2018.</small></li>
					</ul>
				</section>
			</section>
		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script src="plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,
			slideNumber: 'c/t',
			math: {
				mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				config: 'TeX-AMS_HTML-full',
				// pass other options into `MathJax.Hub.Config()`
				TeX: { Macros: { RR: "{\\bf R}" } }
			},
			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
		});
	</script>
</body>

</html>