<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Avanc√©es en Vision Neuromorphique : Repr√©sentation √âv√©nementielle, R√©seaux de Neurones Impulsionnels
		Supervis√©s et Pr√©-entra√Ænement Auto-supervis√©</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/white.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
	<link rel="stylesheet" href="css/w3.css">

	<style>
		.reveal .slide-number {
			font-size: 40px;
		}

		.ita {
			font-style: italic;
		}

		h1,
		h2,
		h3,
		h4,
		h5 {
			text-transform: none !important;
		}

		.grid-container {
			display: grid;
			grid-template-columns: repeat(2, 1fr);
			grid-template-rows: 1fr;
			grid-column-gap: 20px;
			grid-row-gap: 0px;
		}
			
		.div1 { grid-area: 1 / 1 / 2 / 2; }
		.div2 { grid-area: 1 / 2 / 2 / 3; }

		.grid-container3 {
			display: grid;
			grid-template-columns: repeat(2, 1fr);
			grid-template-rows: repeat(2, 1fr);
			grid-column-gap: 20px;
			grid-row-gap: 10px;
		}
		.div1-3 { grid-area: 1 / 1 / 3 / 2; }
		.div3 { grid-area: 2 / 2 / 3 / 3; }

		.container1x3 {
			display: grid;
			grid-template-columns: repeat(3, 1fr);
			grid-template-rows: 1fr;
			grid-column-gap: 0px;
			grid-row-gap: 0px;
			}
			
		.div3-1x3 { grid-area: 1 / 3 / 2 / 4; }

		.container3x3 {
			display: grid;
			grid-template-columns: repeat(3, 1fr);
			grid-template-rows: repeat(2, 1fr);
			grid-column-gap: 0px;
			grid-row-gap: 5px;
		}
			
		.div1-3x3 { grid-area: 1 / 1 / 2 / 2; }
		.div2-3x3 { grid-area: 1 / 2 / 2 / 3; }
		.div3-3x3 { grid-area: 1 / 3 / 2 / 4; }
		.div4-3x3 { grid-area: 2 / 1 / 3 / 2; }
		.div5-3x3 { grid-area: 2 / 2 / 3 / 3; }
		.div6-3x3 { grid-area: 2 / 3 / 3 / 4; }
			
			
					
	</style>
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<!-- Titre -->

			<section>
				<img src="these/entree.png" alt="ent">
			</section>

			<section>
				<section>
					<h1>Introduction</h1>
				</section>

				<section>
					<h3>Vision Artificielle</h3>
					<p>
						üìñ &nbsp; Extraire automatiquement des <b>informations</b> √† partir de <b>donn√©es visuelles</b>
					</p>
					<div class="r-stack">
						<figure class="fragment fade-in-then-out" data-fragment-index="1">
							<img src="these/detection_exemple.png" alt="detection">
							<!-- <figcaption>D√©tection d'objets</figcaption> -->
						</figure>

						<!-- <figure class="fragment fade-in-then-out" data-fragment-index="2">
							<img src="these/vslam.gif" alt="detection">
							<figcaption><small>[Bokovoy2019]</small></figcaption>
						</figure> -->

						<ul class="fragment fade-in-then-out" data-fragment-index="2">
							<li><b>Applications nombreuses :</b> m√©dical, industriel, s√©curit√©, robotique, ...</li>
							<br>
							<li><b>Comment ?</b> R√©seaux de Neurones Artificiels ( <b><span class="ita">ANN</span>s</b>
								) par l'apprentissage profond </li>
							<br>
							<li>üìà &nbsp; <b>Complexit√© des r√©seaux</b></li>
							<ul>
								<li>üìà &nbsp; Puissance de calculs requise $\Rightarrow$ <b>Consommation √©nerg√©tique</b><small>[Desislavov2023]</small></li>
							</ul>
						</ul>
					</div>
				</section>

				<section>
					<h3>Technologie Neuromorphique</h3>
					<p>üìñ &nbsp; Technologie inspir√©e par le fonctionnement des neurones biologiques.</p>
					<br>
					<ul class="fragment">
						<li><b>Capteur :</b> Cam√©ra √©v√©nementielle</li>
						<li><b>Traitement :</b> R√©seaux de neurones impulsionnels (<b><span
									class="ita">SNN</span></b>)</li>
						<br>
						<li class="fragment" style="color:green">Syst√®mes de vision √©conomes en √©nergie </li>
					</ul>
				</section>

				<section>
					<h3>Cam√©ra √âv√©nementielle</h3>
					<ul>
						<li>Inspir√©e de la biologie</li>
						<li>√âv√©nements <b>asynchrones</b> lors d'un changement d'intensit√© du pixel</li>
						<!-- <br> -->
						<li><b>Avantages :</b> faible latence, haute plage dynamique, <b style="color: green;">efficacit√© √©nerg√©tique</b>, ... </li>
					</ul>

					<div class="grid-container">
						<div class="div1"><img src="these/dvs.gif" alt="dvs" height="300px"></div>
						<div class="div2"><img src="img/ssl/lift.gif" alt="enothing" height="350px"></div>
					</div>
					<!-- <table>
						<tr>
							<td><video style="display: inline !important;"  height="300px" muted controls></video> <img src="these/dvs.gif" alt=""></td>
							<td><img src="img/ssl/lift.gif" alt="enothing" height="350px" style="margin-bottom: 250px;"></td>
						</tr>
					</table> -->
				</section>

				<section>
					<h3>R√©seaux de Neurones Impulsionnels</h3>
					<ul>
						<li>Bio-inspir√©s <span class="ita">(neurones impulsionnels)</span></li>
						<li>Neurones communiquent par <b>impulsions binaires</b> dans le temps</li>
						<li><b style="color: green;">Tr√®s basse consommation</b></li>
					</ul>
					<br>
					<div><img src="these/snn.gif" alt="snn" height="330px" style="margin: 0;"></div>
					<p><small>Source : <a
								href="https://www.youtube.com/watch?v=oG0PTP3ogCA">https://www.youtube.com/watch?v=oG0PTP3ogCA</a></small>
					</p>
				</section>

				<section>
					<h3>Probl√©matiques du Neuromorphique</h3>
					<p style="color: red; border: solid 3px;"><b>Domaine attractif mais moins d√©velopp√© que la vision classique</b></p>
					<ul>
						<li class="fragment" data-fragment-index="0"><b>R√©seaux de neurones impulsionnels</b></li>
						<ul class="fragment" data-fragment-index="0">
							<li>Tr√®s prometteurs</li>
							<li>Moins matures que les ANNs <span class="ita">(2020)</span> <b style="color: red;">$\Rightarrow$ peu de diversit√© des t√¢ches de vision</b></li>
						</ul>

						<li class="fragment" data-fragment-index="1"><b>Vision √©v√©nementielle</b></li>
						<ul class="fragment" data-fragment-index="1">
							<li>Int√©r√™t au-del√† de la recherche</li>
							<li>Diversification croissante des applications <b style="color: red">$\Rightarrow$ besoins de BDDs d'apprentissage</b></li>
						</ul>
					</ul>
				</section>

				<!-- <section>
					<h3>Contributions</h3>
					<ul>
						<li><b>Vision √âv√©nementielle</b></li>
						<ul>
							<li>Construire une BDD est un processus co√ªteux <span style="color: red;">$\Rightarrow$ ralentit les progr√®s du domaine</span></li>
							<li class="fragment" style="border: 3px; color:blue;"><b>1√®re contribution :</b> r√©duction du besoins en annotations en vision √©v√©nementielle</li>
						</ul>
						<li><b>SNNs :</b> <span class="ita">(d√©but de th√®se - 2020)</span></li>
						<ul>
							<li>T√¢ches de vision simplistes <span class="ita">(classifications)</span></li>
							<li>√âmergence de SNNs profonds<small>[Neftci2019]</small></li>
							<li class="fragment" style="border: 3px; color:blue;"><b>2√®me contribution :</b> aller vers des t√¢ches de vision plus complexes <b>(+ analyses)</b></li>
						</ul>
					</ul>
				</section> -->
			</section>

			<section>
				<section>
					<h1>√âtat de l'Art</h1>
				</section>

				<!-- <section>
					<h3>Trois Domaines</h3>
					<ul>
						<li><b>Approches classiques</b> : ANN + images</li>
						<br>
						<li><b>Vision √©v√©nementielle</b> : vision artificielle avec des cam√©ras
							√©v√©nementielles</li>
						<br>
						<li><b>R√©seaux de neurones impulsionnels</b></li>
					</ul>
				</section> -->
				<section>
					<h3>Donn√©e - Cam√©ra √âv√©nementielle</h3>
					<div class="r-stack">
						<img src="img/ssl/lift.gif" class="fragment fade-out" data-fragment-index="0" alt="enothing">
						<img src="img/spiking_fer/eventcamintro.png" alt="eventcam" data-fragment-index="0" class="fragment fade-in-then-out">
						<img class="fragment fade-in-then-out" src="img/spiking_fer/eventcamintro_0.png" alt="eventcam" data-fragment-index="1">
						<img class="fragment fade-in-then-out" src="img/spiking_fer/eventcamintro_1.png" alt="eventcam" data-fragment-index="2">
						<img class="fragment fade-in-then-out" src="img/spiking_fer/eventcamintro_2.png" alt="eventcam" data-fragment-index="3">
						<figure class="fragment" data-fragment-index="4">
							<p>Discr√©tisation sur $T$ √©tapes temporelles</p>
							<img src="these/discretisation.png" alt="binary">
							<br>
							<br>
							<figcaption>Tenseur impulsionnel $\mathbf{X}_T \in \mathbb{B}^{T \times C \times H \times W}$</figcaption>
						</figure>
					</div>
				</section>

				<section>
					<h3>Donn√©e - Cam√©ra √âv√©nementielle</h3>
					<p>Deux cat√©gories selon la dynamique de la sc√®ne</p>

					<div class="grid-container">
						<div class="div1">
							<p><b>Comportement <br> statique</b></p>
							<img src="these/sota/nmnist.gif" alt="simu" height="360px">
						</div>
						<div class="div2">
							<p><b>Comportement dynamique</b></p>
							<img src="these/sota/dailyactiondvs.gif" alt="enothing">
						</div>
					</div> 
				</section>

				<section>
					<h3>Donn√©e - Image Statique</h3>
					<div class="r-stack">
						<div class="fragment fade-out" data-fragment-index="0">
							<p style="color: red;">‚ùå &nbsp; Les images statiques ($\in \mathbb{R}$) ne sont pas adapt√©s pour le traitement par des neurones impulsionnels</p>
							<br>
							<img src="these/sota/codage.png" alt="codage">
						</div>

						<div class="container1x3 fragment" data-fragment-index="0">
							<div class="div1">
								<figure style="margin: 0;">
									<figcaption>&nbsp;</figcaption>
									<img src="these/localization/nc/original.png" alt="origi" height="300px" style="margin: 0;">
								</figure>
							</div>
							<div class="div2">
								<figure style="margin: 0;">
									<figcaption>Fr√©quentiel</figcaption>
									<img src="these/localization/nc/rate.gif" alt="nc" height="300px" style="margin: 0;">
								</figure>
							</div>
							<div class="div3-1x3">
								<figure style="margin: 0;">
									<figcaption>Temporel</figcaption>
									<img src="these/localization/nc/ttfs.gif" alt="nc" height="300px" style="margin: 0;">
								</figure>
							</div>
						</div>
					</div>
				</section>

				<section>
					<h3>Apprentissage Profond</h3>
					<ul>
						<li><b>R√©seaux de neurones convolutifs <span class="ita">(CNNs)</span><small>[Kzh2012]</small></b></li>
						<ul>
							<li><b>2D-CNN</b> : convolutions 2D pour les images</li>
							<li><b>3D-CNN</b> : convolutions 3D pour les vid√©os</li>
						</ul>
						<img src="these/convenc.png" alt="convenc">	
					</ul>
				</section>

				<section>
					<h3>Apprentissage Profond et √âv√©nements</h3>
					<ul>
						<li><b>ANNs sont tr√®s courants</b> pour traiter les √©v√©nements<small>[Zheng2023]</small></li>
						<li>üìà &nbsp; <b>T√¢ches de vision</b></li>
						<ul>
							<li>Classification simple<small>[Orchard2015]</small> $\Rightarrow$ Synth√®se de vues 3D <small>[Rudnev2023]</small></li>
						</ul>	
						<li>üìà &nbsp; <b>Contextes</b></li>
						<ul>
							<li>i.e. Conduite autonome<small>[Cordone2022]</small>, d√©tection de fatigue<small>[Kielty2023]</small></li>
						</ul>
						<li>üìà &nbsp; <b>Architectures</b></li>
							<ul>
							<li>2D-CNN<small>[Gallego2020]</small> $\Rightarrow$ Transformeur de vision (ViT)<small>[Gehrig2023]</small></li>
						</ul>
					</ul>
				</section>

				<section>
					<h3>R√©seaux de Neurones Impulsionnels</h3>
					<br>
					<p><b>Neurone artificiel</b> &nbsp; $vs.$ &nbsp; <b>Neurone Impulsionnel</b></p>
					<br>
					<div class="r-stack">
						<img src="these/sota/artificial_impulsion_0.png" alt="annvssnn">
						<img src="these/sota/artificial_impulsion_1.png" alt="annvssnn" class="fragment">
						<img src="these/sota/artificial_impulsion_2.png" alt="annvssnn" class="fragment">
						<img src="these/sota/artificial_impulsion.png" alt="annvssnn" class="fragment">
					</div>
				</section>

				<section>
					<h3>Neurone "Integrate-and-Fire" (IF)</h3>
					<p><small>[Lapicque1907]</small></p>
					<img src="these/sota/if_dyna.png" alt="dyna">
				</section>

				<section>
					<h3>R√®gles d'Apprentissage</h3>
					<div class="r-stack">
						<ul class="fragment fade-out" data-fragment-index="0">
							<li><b>Conversion ANN-vers-SNN</b></li>
							<ul>
								<li style="list-style: '‚úÖ';"> &nbsp; SNNs profonds et performants</li>
								<li style="list-style: '‚ùå';">&nbsp; Lents et consommateurs</li>
							</ul>
							<br>
							<li><b>R√®gles d'apprentissage biologique</b> <span class="ita">(STDP)</span></li>
							<ul>
								<li style="list-style: '‚úÖ';"> &nbsp; Non-supervis√©, adapt√© au mat√©riel sp√©cialis√©</li>
								<li style="list-style: '‚ùå';">&nbsp; Complexit√© des SNNs limit√©e </li>
							</ul>
						</ul>

						<ul class="fragment fade-in" data-fragment-index="0">
							<li><b>R√©tropropagation</b> <span class="fragment" data-fragment-index="1">&nbsp; üèÖ</span></li>
							<ul>
								<li>SNN $=$ R√©seau de neurones r√©currents</li>
								<li>$\Rightarrow$ R√©tropropagation √† travers le temps</li>
								<br>
								<li class="fragment" data-fragment-index="2" >‚úÖ &nbsp; Apprentissage direct du SNN</li>
								<li class="fragment" data-fragment-index="2" >‚úÖ &nbsp; SNNs profonds et performants</li>
								<br>
								<li class="fragment" data-fragment-index="3" style="color: orange;">‚ö†Ô∏è &nbsp; Non-diff√©rentiabilit√© des impulsions</li>
								<li class="fragment" data-fragment-index="3" style="color: green;"><b>$\Rightarrow$ Apprentissage par Substitut du Gradient</b> (SG)<small>[Neftci2019]</small></li>
							</ul>
						</ul>
					</div>
				</section>

				<section>
					<h3>Apprentissage par Substitut du Gradient</h3>
					<div class="r-stack">
						<div class="fragment fade-out" data-fragment-index="0">
							<p style="color: red;">Probl√®me du neurone mort<small>[Eshraghian2021]</small></p>
							<img src="these/heaviside_simple_0.png" alt="go">
						</div>
						<div class="fragment fade-in-then-out" data-fragment-index="0">
							<p style="color: red;">Probl√®me du neurone mort<small>[Eshraghian2021]</small></p>
							<img src="these/heaviside_simple_1.png" alt="go">
						</div>
						<div class="fragment fade-in-then-out" data-fragment-index="1">
							<p style="color: red;">Probl√®me du neurone mort<small>[Eshraghian2021]</small></p>
							<img src="these/heaviside_simple.png" alt="go">
						</div>
						<div class="fragment fade-in-then-out" data-fragment-index="2">
							<p style="color: green;">Remplacer la d√©riv√©e par un <b>substitut</b></p>
							<img src="these/sg_simple.png" alt="go">
						</div>
					</div>
				</section>

				<section>
					<h3>SNNs Profonds en Vision</h3>
					<ul>
						<li><b>D√©but de th√®se </b>(2020-2021)</li>
						<ul>
							<li>√âmergence des architectures profondes <br><span class="ita" style="font-size: 0.8em;">(SEW-ResNet<small>[Fang2021]</small>, ...)</span></li>
							<li>Hautes performances en classification <br><span class="ita" style="font-size: 0.8em;">(CIFAR10-DVS<small>[Li2017]</small>, ...)</span></li>
						</ul>
						<li style="list-style: none;">$\Rightarrow$ <b>Prometteur pour aller plus loin</b> </li>
						<br>
						<li class="fragment" data-fragment-index="0"><b>Actuellement</b></li>
						<ul class="fragment" data-fragment-index="0">
							<li>i.e. segmentation<small>[K2022]</small>, d√©tection<small>[Crd2022]</small></li>
							<li>Versions impulsionnelles d'architectures profondes (ViT<small>[Zhou2022]</small>, SSD<small>[Crd2022]</small>)</li>
						</ul>
					</ul>
				</section>

				<section>
					<h3>Positionnement des Contributions</h3>
					<ol>
						<li><b>D√©but de th√®se :</b></li>
						<ul>
							<li><b>Aller plus loin</b> que la classification ?</li>
							<li><b>Analyser le comportement</b> ces nouveaux SNNs</li>
							<li class="fragment" style="color: blue;"><b>Explorer et analyser la localisation d'objet avec les SNNs entra√Æn√©s par SG</b></li>
						</ul>
						<br>
						<li><b>Progr√®s en vision √©v√©nementielle</b></li>
						<ul>
							<li>Comment <b>faciliter </b> le d√©veloppement dans de <b>nouveaux contextes</b> ?</li>
							<li class="fragment" style="color: blue;"><b>M√©thode de pr√©-entra√Ænement par apprentissage auto-supervis√©</b></li>
						</ul>
					</ol>
				</section>
			</section>

			<section>
				<section>
					
					<h2>√âtude des R√©seaux de Neurones Impulsionnels <br> par la Localisation d'Objet</h2>
				</section>

				<section>
					<h3>Analyses des SNNs</h3>
					<ul>
						<li><b>Historiquement</b> (SNNs peu profonds avec STDP) <b>:</b></li>
						<ul>
							<li><b>Images statiques : </b>sup√©riorit√© du codage temporel<small>[Falez2019]</small></li>
							<li><b>Latence temporelle</b> ($T$) <b>:</b> une grande valeur $T$ est pr√©f√©rable<small>[Guo2021]</small></li>
							<li><b>Robustesse aux corruptions</b><small>[Guo2021]</small></li>
						</ul>
						<br>
						<li class="fragment" data-fragment-index="0" style="color: darkorange;">‚ö†Ô∏è &nbsp; Est-ce que l'apprentissage par SG est diff√©rent ?</li>
						<ul class="fragment" data-fragment-index="0">
							<li style="color: red;">R√©ponse inconnue</li>
							<li style="list-style-type: none;">$\Rightarrow$ <b>√âtudes exp√©rimentales</b></li>
						</ul>
					</ul>
				</section>

				<section>
					<h3>Formulation - Localisation d'Objet</h3>
					<div class="r-stack">
						<img src="these/localization/localization_formulation.png" alt="local forma">
						<!-- <div class="fragment">
							<p><b>Pourquoi cette t√¢che ?</b></p>
							<ul style="list-style-type: '‚úîÔ∏è&nbsp;  ';">
								<li>Traitement de l'information spatiale</li>
								<li>Plus simple que des t√¢ches similaires (<span class="ita">d√©tection d'objets, ...</span>)</li>
								<li>Sc√®nes complexes</li>
							</ul>
						</div> -->
					</div>
					<p></p>
				</section>

				<section>
					<h3>Contributions</h3>
					<ul>
						<li><b>Deux modalit√©s :</b> images statiques et √©v√©nements</li>
						<br>
						<li><b>Influence de trois param√®tres sur les performances</b></li>
						<ol>
							<li><b>Latence temporelle</b></li>
							<li>Corruptions des capteurs $\Rightarrow$ <b>robustesse</b></li>
							<li>Le <b>codage neuronal</b> d'une image statique</li>
						</ol>
						<br>
						<li>Estimation du co√ªt √©nerg√©tique</li>
						<br>
						<li><b>Comparaison syst√©matique avec un ANN</b></li>
					</ul>
				</section>

				<section>
					<h3>Encodeur Convolutif - ANN</h3>
					<div class="r-stack">
						<img src="these/localization/ann_archi.png" alt="ANN">
					</div>
				</section>

				<section>
					<h3>Encodeur Convolutif - SNN</h3>
					<div class="r-stack">
						<img src="these/localization/snn_archi_0.png" alt="SNN">
						<img src="these/localization/snn_archi_1.png" alt="SNN" class="fragment">
						<img src="these/localization/snn_archi_2.png" alt="SNN" class="fragment">
						<img src="these/localization/snn_archi_3.png" alt="SNN" class="fragment">
						<img src="these/localization/snn_archi_4.png" alt="SNN" class="fragment">
						<img src="these/localization/snn_archi.png" alt="SNN" class="fragment">
					</div>
				</section>

				<section>
					<h3>Bases de Donn√©es</h3>
					<img src="these/localization/bdds_loca.png" alt="bdd loca" height="350px">
					<img style="margin-top: 0;" src="these/localization/datasets.png" alt="data">
				</section>

				<section>
					<h3>Codages Neuronaux √âtudi√©s</h3>
					<div class="container3x3">
						<div class="div1-3x3">
							<figure style="margin: 0;">
								<figcaption>&nbsp;</figcaption>
								<img src="these/localization/nc/original.png" alt="origi" height="250px" style="margin: 0;">
							</figure>
						</div>
						<div class="div2-3x3">
							<figure style="margin: 0;">
								<figcaption>Fr√©quentiel</figcaption>
								<img src="these/localization/nc/rate.gif" alt="origi" height="250px" style="margin: 0;">
							</figure>
						</div>
						<div class="div3-3x3">
							<figure style="margin: 0;">
								<figcaption>Temporel</figcaption>
								<img src="these/localization/nc/ttfs.gif" alt="origi" height="250px" style="margin: 0;">
							</figure>
						</div>
						<div class="div4-3x3">
							<figure style="margin: 0;">
								<img src="these/localization/nc/phase.gif" alt="origi" height="250px" style="margin: 0;">
								<figcaption>par Phases</figcaption>
							</figure>
						</div>
						<div class="div5-3x3">
							<figure style="margin: 0;">
								<img src="these/localization/nc/saccade.gif" alt="origi" height="250px" style="margin: 0;">
								<figcaption>par Saccades</figcaption>
							</figure>
						</div>
						<div class="div6-3x3">
							<div style="border: 2px solid; height: 250px; width: 250px; margin-left: 35px;">
								<br>
								<p>Codage entra√Ænable</p>
								<br>
							</div>
						</div>
					</div> 
					<!-- <table>
						<tr>
							<td>
								<img src="these/localization/nc/original.png" alt="origi" height="375px">
								<p style="text-align: center;">
									Original
								</p>
							</td>
							<td>
								<div class="r-stack">
									<div class="fragment fade-in-then-out">
										<img src="these/localization/nc/rate.gif" alt="nc" height="400px">
										<p style="text-align: center; margin-top: 5px;"><b>Codage fr√©quentiel</b></p>
									</div>
									<div class="fragment fade-in-then-out">
										<img src="these/localization/nc/ttfs.gif" alt="nc" height="400px">
										<p style="text-align: center; margin-top: 5px;"><b>Codage temporel</b></p>
									</div>
									<div class="fragment fade-in-then-out">
										<img src="these/localization/nc/phase.gif" alt="nc" height="400px">
										<p style="text-align: center; margin-top: 5px;"><b>Codage par phases</b></p>
									</div>
									<div class="fragment fade-in-then-out">
										<img src="these/localization/nc/saccade.gif" alt="nc" height="400px">
										<p style="text-align: center; margin-top: 5px;"><b>üÜï &nbsp; Codage par saccades</b></p>
									</div>
									<div class="fragment fade-in-then-out">
										<p style="text-align: center; margin-top: 5px;"><b>Codage entra√Ænable</b></p>
									</div>
								</div>
							</td>
						</tr>
					</table> -->
				</section>

				<section>
					<h3>√âtude sur la Latence Temporelle</h3>
					<p><b>Protocole</b></p>
						<ol>
							<li>D√©finir un nombre $T$ d'√©tapes temporelles</li>
							<br>
							<li>Effectuer un entra√Ænement</li>
							<br>
							<li>Mesurer la performance de localisation (% $mIoU$) sur l'ensemble de validation</li>
						</ol>
				</section>

				<section>
					<h3>Latence Temporelle - Images Statiques</h3>
					<div class="r-stack">
						<img src="these/localization/images_latences_0.png" alt="latence image" class="fragment fade-out" data-fragment-index="0">
						<img src="these/localization/images_latences_1.png" alt="latence image" class="fragment fade-in-then-out" data-fragment-index="0">
						<img src="these/localization/images_latences_2.png" alt="latence image" class="fragment fade-in-then-out" data-fragment-index="1">
						<img src="these/localization/images_latences_3.png" alt="latence image" class="fragment fade-in-then-out" data-fragment-index="2">
					</div>
					<ul>
						<li class="fragment" data-fragment-index="0">Aucune correlation significative</li>
						<li class="fragment" data-fragment-index="1">SNN comp√©titif (au mieux, $\Delta = 2.97$%)</li>
						<li class="fragment" data-fragment-index="2"><b>Contraire au STDP :</b> codage temporel $<<$</li>
					</ul>
				</section>

				<section>
					<h3>Latence Temporelle - √âv√©nements</h3>
					<div class="r-stack">
						<img src="these/localization/latence_events.png" alt="latence events" class="fragment fade-out" data-fragment-index="0">
						<img src="these/localization/latence_events_1.png" alt="latence events" class="fragment" data-fragment-index="0">
						<img src="these/localization/latence_events_2.png" alt="latence events" class="fragment" data-fragment-index="1">
						<img src="these/localization/latence_events_3.png" alt="latence events" class="fragment" data-fragment-index="2">
					</div>
					<ul>
						<li class="fragment" data-fragment-index="0"><b>ANN :</b> Les performances restent constantes</li>
						<li class="fragment" data-fragment-index="1"><b>SNN :</b> &nbsp; $T$ üìâ &nbsp; $\rightarrow$ performances &nbsp; üìà</li>
						<li class="fragment" data-fragment-index="2">√Ä partir de $T=8$, le SNN est meilleur</li>
					</ul>
				</section>

				<section>
					<h3>√âtude sur la Robustesse aux Corruptions</h3>
					<p><b>Protocole</b></p>
					<div class="r-stack">
						<img src="these/localization/img_corrup/protocol_0.png" alt="corr proto">
						<img src="these/localization/img_corrup/protocol_1.png" alt="corr proto" class="fragment fade-in-then-out">
						<img src="these/localization/img_corrup/protocol_2.png" alt="corr proto" class="fragment fade-in-then-out">
						<img src="these/localization/img_corrup/protocol_3.png" alt="corr proto" class="fragment fade-in-then-out">
						<img src="these/localization/img_corrup/protocol.png" alt="corr proto" class="fragment fade-in-then-out">
						<img src="these/localization/img_corrup/protocol_5.png" alt="corr proto" class="fragment fade-in-then-out">
					</div>
				</section>


				<section>
					<h3>Robustesse - Images Statiques</h3>
					<img src="these/localization/corruptions_images.png" alt="corruptions">
				</section>

				<section>
					<h3>Images Statiques - Corruptions</h3>
					<!-- <p>Valeur de $mRAD^{corr}$ pour chaque corruption et chaque codage neuronal</p> -->
					<div class="r-stack">
						<img src="these/localization/corruptions_charts.png" alt="mrad" height="450px" style="margin-top: 0;">
						<img src="these/localization/corruptions_charts_0.png" alt="mrad" height="450px" class="fragment" data-fragment-index="0" style="margin-top: 0;">
						<img src="these/localization/corruptions_charts_1.png" alt="mrad" height="450px" class="fragment" data-fragment-index="1" style="margin-top: 0;">
						<img src="these/localization/corruptions_charts_2.png" alt="mrad" height="450px" class="fragment" data-fragment-index="2" style="margin-top: 0;">
						<img src="these/localization/corruptions_charts_3.png" alt="mrad" height="450px" class="fragment" data-fragment-index="3" style="margin-top: 0;">
					</div>
					<ul>
						<li class="fragment" data-fragment-index="1"><b>Temporel :</b> tr√®s robuste sauf pour le givre</li>
						<li class="fragment" data-fragment-index="2"><b>Saccades :</b> contraire observ√©</li>
						<li class="fragment" data-fragment-index="3" style="color: green;"><b>Fr√©quentiel :</b> grande robustesse g√©n√©rale</li>
					</ul>
				</section>

				<section>
					<h3>Robustesse - √âv√©nements</h3>
					<div class="grid-container3">
						<div class="div1-3" style="margin-top: 50px;"> 
							<figure style="margin: 0;">
								<figcaption>Original</figcaption>
								<img src="these/localization/img_corrup/normal.gif" alt="normal" height="400px" style="margin: 0;">
							</figure>
							
						</div>
						<div class="div2">
							<figure style="margin: 0;">
								<figcaption style="font-size: 30px;">Bruit d'activit√© de fond</figcaption>
								<img src="these/localization/img_corrup/baa.gif" alt="k√©" height="250px" style="margin: 0;">
							</figure>
						</div>
						<div class="div3">
							<figure style="margin: 0;">
								<img src="these/localization/img_corrup/hotpix.gif" alt="k√©" style="margin: 0;" height="250px">
								<figcaption style="font-size: 30px;">Bruit "hot pixels"</figcaption>
							</figure>
						</div>
					</div> 

				</section>


				<section>
					<h3>Robustesse - √âv√©nements</h3>
					<!-- <p>Valeur de $mRAD^{corr}$ pour chaque corruption</p> -->
					<img src="these/localization/event_chart_corr.png" alt="corruption" height="375px">
					<ul>
						<li>Sensibilit√© au bruit du SNN $\rightarrow$ hypoth√®se des potentiels de membrane</li>
					</ul>
				</section>

				<section>
					<h3>Consommation √ânerg√©tique</h3>
					<div class="r-stack">
						<ul class="fragment fade-out" data-fragment-index="0">
							<li><b>Estimation de l'√©nergie consomm√©e</b><small>[K2022]</small></li>
							<br>
							<ul>
								<li>Calcul des FLOPs effectu√©s lors de l'inf√©rence</li>
								<li style="list-style: none;">‚ÑπÔ∏è &nbsp; <b><span class="ita">(li√© au nombre d'impulsions √©mises)</span></b></li>
								<br>
								<li>Estimation sur une puce CMOS de 45nm <small>[Horowitz2024]</small> :</li>
								<ul>
									<li>$E_{ANN} \rightarrow$ √©nergie consomm√©e par l'ANN <span class="ita">(en mJ)</span></li>
									<li>$E_{SNN} \rightarrow$ √©nergie consomm√©e par le SNN</li>
								</ul>
								<br>
								<li style="list-style: none;">$\Rightarrow$ <b>On reporte le ratio :</b> $\frac{E_{ANN}}{E_{SNN}}$</li>
							</ul>
						</ul>
						<div class="fragment" data-fragment-index="0">
							<img src="these/localization/conso_bars.png" alt="conso bars">
							<ul>
								<li class="fragment" data-fragment-index="1"><b>Net avantage pour les SNNs</b></li>
								<li class="fragment" data-fragment-index="1">Les flux d'√©v√©nements sont comparables aux codages neuronaux intensifs</li>
							</ul>
						</div>
					</div>
				</section>

				<section>
					<h3>Bilan de l'√âtude</h3>
					<ul>
						<li class="fragment" data-fragment-index="0"><b>Sup√©riorit√© des faibles latences</b></li>
						<ul class="fragment" data-fragment-index="0">
							<li>SNN plus rapide, plus √©conome, voire plus performant</li>
						</ul>
						<br>
						<li class="fragment" data-fragment-index="1"><b>Codages Neuronaux : </b> int√©r√™t du <span style="color: green;"><b>codage fr√©quentiel</b></span></li>
						<ul class="fragment" data-fragment-index="1">
							<li>Performant et robuste</li>
						</ul>
						<br>
						<li class="fragment" data-fragment-index="2"><b>√âv√©nements :</b></li>
						<ul class="fragment" data-fragment-index="2">
							<li>üèÖ &nbsp; <b>SNN plus performant que ANN</b></li>
							<li>... mais moins robuste</li>
						</ul>
					</ul>
				</section>

				<section>
					<h3>Conclusion</h3>
					<ul>
						<li>Aller au-del√† de la classification <b>est possible</b></li>
						<br>
						<li  class="fragment" data-fragment-index="2"><b>Contradictions avec les √©tudes pr√©c√©dentes</b> <span class="ita">(STDP)</span><small>[Falez2019,Guo2021]</small></li>
						<ul  class="fragment" data-fragment-index="2">
							<li>Faiblesse du <span style="color:red;">codage temporel</span></li>
							<li>Influence de la latence temporelle</li>
							<li><b>$\Rightarrow$ Remise en cause des connaissances</b></li>
						</ul>
					</ul>
					<br>
					<br>
					<br>
					<!-- <p style="text-align: justify;"><small>üìî &nbsp; <span class="ita">"Deep spiking convolutional neural network for single object localization based on deep continuous local learning."</span> <b>2021 International Conference on Content-Based Multimedia Indexing (CBMI). IEEE, 2021.</b></small></p> -->
					<p style="border-top: solid 2px; padding-top: 5px; text-align: justify;"><small>üìî &nbsp; <span class="ita">"Spiking neural networks for frame-based and event-based single object localization."</span> <b>Neurocomputing 559 (2023): 126805.</b></small></p>
				</section>
			</section>

			<section>
				<section>
					<h2>Pr√©-entra√Ænement <br> Auto-supervis√© <br> pour la Vision
							√âv√©nementielle</h2>
				</section>

				<!-- <section>
					<h3>Contexte</h3>
					<p style="color: blue; border: solid 3px; font-weight: bold; padding: 10px;">üéØ &nbsp; R√©duire le besoin en donn√©es annot√©es pour la vision √©v√©nementielle</p>
					<ul>
						<li>üìà &nbsp; Mod√®les profonds pour la vision √©v√©nementielle</li>
						<br>
						<li><b>Apprentissage supervis√© :</b> n√©cessite beaucoup de donn√©es annot√©es</li>
						<br>
						<li style="color:red;" class="fragment" data-fragment-index="0">
							Complexifie le d√©veloppement de nouvelles applications
						</li>
					</ul>
					<br>
					<br>
					<p class="fragment" style="font-weight: bold; border: 3px solid; color: black; padding-top: 5px;">üó®Ô∏è &nbsp; Comment r√©duire le besoin en annotations pour les mod√®les profonds en vision √©v√©nementielle ?</p>
				</section> -->

				<section>
					<h3>R√©duire le Besoin en Annotations</h3>
					<div class="r-stack">
						<div class="fragment fade-in-then-out" data-fragment-index="4">
							<img src="these/pretraining_supervised.png" alt="pretraining">
							<ul>
								<li><b>Supervis√© :</b> utiliser une grande BDD <span class="ita">g√©n√©rique</span>
									annot√©e puis affiner</li>
								<ul>
									<li style="color: red;">‚ùå &nbsp; peu de BDDs √©v√©nementielles pertinentes</li>
								</ul>
							</ul>
						</div>

						<div class="fragment fade-in-then-out" data-fragment-index="5">
							<img src="these/pretraining_ssl.png" alt="pretraining">
							<ul>
								<li><b>Apprentissage Auto-supervis√© de Repr√©sentation <span class="ita">(SSRL)</span> :</b> capturer les propri√©t√©s et motifs intrins√®ques des donn√©es </li>
								<ul style="color: green;">
									<li>‚úÖ &nbsp; Pas d'annotations requises</li>
									<li>‚úÖ &nbsp; Proche du domaine d'application</li>
								</ul>
							</ul>
						</div>
					</div>
				</section>

				<section>
					<h3>SSRL √©v√©nementiel</h3>
					<ul>
						<li><b>Trois travaux similaires</b> <span class="ita">(2022 - 2023)</span><small>[Li2022,Klenk2022,Yang2023]</small></li>
						<ul>
							<li>‚ùå &nbsp; Limit√©s √† du <span style="color: red;">comportement statique</span></li>
							<li>‚ùå &nbsp; Exp√©rimentations : <span style="color: red;">√©valuations diff√©rentes</span></li>
							<li>‚ùå &nbsp; Concentr√©s sur <span style="color: red;">un type de r√©seau</span> <span class="ita">(ViT / SNN)</span></li>
							<li>‚ùå &nbsp; Les mod√®les √©tudi√©s sont <span style="color: red;">lourds</span></li>
						</ul>
						<br>
						<li class="fragment" data-fragment-index="0"><b>Constat</b></li>
						<ul class="fragment" data-fragment-index="0">
							<li>Domaine <b style="color: green;">prometteur</b> mais encore <b style="color: red;">sous-exploit√©</b></li>
						</ul>
					</ul>
				</section>			
				
				<section>
					<h3>Travaux</h3>
					<ul>
						<li><b>Nouvelle m√©thode de r√©f√©rence</b> pour des encodeurs convolutifs l√©gers <span class="ita">(CSNN, 2D-CNN, et 3D-CNN)</span></li>
						<br>
						<li><b>Polyvalence des donn√©es :</b> BDDs √† comportement statique et dynamique</li>
						<br>
						<li><b>Protocoles d'√©valuation standardis√©s</b></li>
						<br>
						<li><b>√âtude sur les augmentations de donn√©es</b> dans le cadre du SSRL</li>
					</ul>
				</section>

				<section>
					<h3>M√©thode</h3>
					<p><b>Augmentation de Donn√©es √âv√©nementielle</b> (EDA)</p>
					<div class="r-stack">
						<img src="img/ssl/dataaug.png" alt="data">
						<img src="img/ssl/dataaug_comp2.png" alt="data" class="fragment" data-fragment-index="0">
					</div>
					<p class="fragment" data-fragment-index="0">‚ÑπÔ∏è &nbsp; Une EDA peut √™tre une <b>composition</b> d'autres EDAs</p>
				</section>

				<section>
					<h3>M√©thode</h3>
					<p><b>Architecture d'Encodage Conjoint</b><small>[Bardes2022][Zbontar2021]</small></p>
					<div class="r-stack">
						<!-- <img src="these/archi_ssl_0.png" alt="archi ssl"> -->
						<img src="these/archi_ssl_1.png" alt="archi ssl">
						<img src="these/archi_ssl_2.png" alt="archi ssl" class="fragment">
						<img src="these/archi_ssl_3.png" alt="archi ssl" class="fragment">
						<img src="these/archi_ssl_3_1.png" alt="archi ssl" class="fragment">
						<img src="these/archi_ssl_4.png" alt="archi ssl" class="fragment">
						<img src="these/archi_ssl_4_1.png" alt="archi ssl" class="fragment">
						<img src="these/archi_ssl.png" alt="archi ssl" class="fragment">
						<img src="these/archi_ssl_5.png" alt="archi ssl" class="fragment">
					</div>
				</section>

				<section>
					<h3>M√©thode - Encodeurs √âtudi√©s</h3>
					<br>
					<ul>
						<li><b>2D-CNN : </b>ResNet-18<small>[He2016]</small></li>
						<li><b>CSNN : </b>SEW-ResNet-18<small>[Fang2021]</small></li>
						<li><b>3D-CNN : </b>MC3-ResNet-18<small>[Tran2017]</small></li>
					</ul>
					<br>
					<br>
					<p>‚ÑπÔ∏è &nbsp; <span class="ita">M√™me architecture et m√™me complexit√©</span></p>
				</section>

				<section>
					<h3>M√©thode - Variantes</h3>
					<br>
					<div class="r-stack">
						<img src="these/variants_ssl_0.png" alt="variantes ssl">
						<img src="these/variants_ssl_1.png" alt="variantes ssl" class="fragment" data-fragment-index="1">
						<img src="these/variants_ssl.png" alt="variantes ssl" class="fragment" data-fragment-index="2">
					</div>
					<ul>
						<li class="fragment" data-fragment-index="1"><b>üë¨ &nbsp; Jumeaux : </b>architecture classique avec poids partag√©s</li>
						<br>
						<li class="fragment" data-fragment-index="2"><b>üë®‚Äçüéìüßë‚Äçüè´ &nbsp; √âtudiant-Professeur : </b>CSNN (<span class="ita">√©tudiant</span>) coupl√© √† 2D-/3D-CNN (<span class="ita">professeur</span>)</li>
					</ul>
				</section>

				<section>
					<h3>M√©thode - Augmentations de Donn√©es</h3>
					<p>√Ä chaque inf√©rence, une composition $d_A$ / $d_B$ est √©chantillonn√©e d'une distribution $D$</p>
					<div class="r-stack">
						<img class="fragment fade-in-then-out" data-fragment-index="0" src="these/archi_ssl_6.png" alt="ssl d">
						<img class="fragment fade-in-then-out" data-fragment-index="1" src="these/eda_distrib_0.png" alt="distrib d">
						<img class="fragment fade-in-then-out" data-fragment-index="2" src="these/eda_distrib_1.png" alt="distrib d">
						<img class="fragment fade-in-then-out" data-fragment-index="3" src="these/eda_distrib_2.png" alt="distrib d">
						<img class="fragment fade-in-then-out" data-fragment-index="4" src="these/eda_distrib.png" alt="distrib d">
						<p class="fragment" data-fragment-index="5" style="color: darkorange;"><b>‚ö†Ô∏è D√©finir une distribution $D$ efficace est essentiel ‚ö†Ô∏è </b></p>
					</div>
				</section>

				<section>
					<h3>Augmentations √âtudi√©es</h3>
					<img src="these/ssrl/edas_summary.png" alt="eda summary">
				</section>

				<section>
					<h3>√âvaluation des Performances</h3>
					<div class="r-stack">
						<div class="fragment fade-out" data-fragment-index="0">
							<p style="border: solid 3px red; margin:10px; color:red; padding-top: 5px;">‚ùå &nbsp; Pas de protocole d'√©valuation commun en SSRL √©v√©nementiel</p>
							<br>
							<ul>
								<li style="color:green;">D√©finir des <b>protocoles d'√©valuation standards</b> pour les travaux futurs</li>
								<ul>
									<li>Probl√®mes de classification $\rightarrow$ taux de pr√©cision</li>
									<li>Trois protocoles pour √©valuer des aspects sp√©cifiques du SSRL</li>
								</ul>
							</ul>
						</div>
						<div class="fragment fade-in" data-fragment-index="0">
							<img height="500px" src="these/ssrl/bdd_ssrl.png" alt="ssrl bdd">
						</div>
					</div>
				</section>

				<section>
					<h3>Protocole n¬∞1 - √âvaluation Lin√©aire</h3>
					<p>üéØ &nbsp; Est-ce que la m√©thode de SSRL <b>extrait des caract√©ristiques pertinentes ?</b></p>
					<br>
					<div class="r-stack">
						<!-- <img src="these/linear.png" alt="linear"> -->
						<img src="these/ssrl/ssrl_linear.png" alt="linear">
					</div>
					<br>
					<br>
				</section>

				<section>
					<h3>Protocole n¬∞2 - Transfert d'Apprentissage</h3>
					<p>üéØ &nbsp; Est-ce que les caract√©ristiques apprises peuvent <b>√™tre transf√©r√©es √† d'autres donn√©es ?</b></p>
					<br>
					<div class="r-stack">
						<img src="these/ssrl/ssrl_transf.png" alt="linear">
					</div>
					<br>
					<br>
				</section>

				<section>
					<h3 style="font-size: 1.1em;">Protocole n¬∞3 - Apprentissage Semi-supervis√©</h3>
					<p>üéØ &nbsp; Est-ce que la m√©thode de SSRL permet de <b>r√©duire le besoin en annotations ?</b></p>
					<br>
					<div class="r-stack">
						<img src="these/ssrl/ssrl_semi.png" alt="linear">
					</div>
					<br>
					<br>
				</section>

				<section>
					<h3>√âtude sur les EDAs</h3>
					<p style="border: 3px solid; font-weight: bold; padding-top: 5px;">üéØ &nbsp; √âvaluer l'int√©r√™t des EDAs pour le SSRL</p>
					<br>
					<ul>
						<li>Protocole d'<b>√©valuation lin√©aire</b> sur <b>DVSGesture</b></li>
						<br>
						<li><b>Trois √©tapes incr√©mentales : </b>une √©tape par cat√©gorie</li>
						<br>
						<li>Pour chaque √©tape, on conserve la combinaison d'EDAs <b>la plus performante de l'√©tape pr√©c√©dente</b></li>
					</ul>
				</section>

				<section>
					<h3>√âtude sur les EDAs</h3>
					<div class="r-stack">
						<p class="fragment fade-out" data-fragment-index="2"><b>√âtape 1 :</b> EDAs Communes</p>
						<p class="fragment fade-in" data-fragment-index="2"><span class="fragment fade-out" data-fragment-index="6"><b>√âtape 2 :</b> EDAs G√©om√©triques</span></p>
						<p class="fragment" data-fragment-index="6"><b>√âtape 3 :</b> EDAs en d√©coupage</p>
					</div>
					<div class="r-stack">
						<img src="these/ssrl/eda_communes_results_0.png" height="450px" alt="eda">
						<img src="these/ssrl/eda_communes_results_1.png" height="450px" alt="eda" class="fragment fade-in-then-out" data-fragment-index="0">
						<img src="these/ssrl/eda_communes_results.png" height="450px" alt="eda" class="fragment fade-in-then-out" data-fragment-index="1">
						<img src="these/ssrl/eda_geo_results_0.png" height="450px" alt="eda" class="fragment fade-in-then-out" data-fragment-index="2">
						<img src="these/ssrl/eda_geo_results_1.png" height="450px" alt="eda" class="fragment fade-in-then-out" data-fragment-index="3">
						<img src="these/ssrl/eda_geo_results_2.png" height="450px" alt="eda" class="fragment fade-in-then-out" data-fragment-index="4">
						<img src="these/ssrl/eda_geo_results.png" height="450px" alt="eda" class="fragment fade-in-then-out" data-fragment-index="5">
						<img src="these/ssrl/eda_drop_results.png" height="450px" alt="eda" class="fragment fade-in" data-fragment-index="6">
					</div>
					<div class="r-stack">
						<div class="fragment" data-fragment-index="1">
							<ul class="fragment fade-out" data-fragment-index="2">
								<li>Une EDA (<span style="color: red;">rouge</span>) &lsaquo; deux EDAs (<span style="color:green;">vert</span>) &lsaquo; Trois EDAs (<span style="color:blue;">bleu</span>)</li>
							</ul>
						</div>
						
						<div class="fragment" data-fragment-index="2">
							<ul class="fragment fade-out" data-fragment-index="6">
								<li>Dynamiques pour 3D-CNN et CSNN</li>
								<li>Statiques pour 2D-CNN</li>
							</ul>
						</div>

						<ul class="fragment fade-in" data-fragment-index="7">
							<li><code>EventCopyDrop</code> est syst√©matiquement ü•á ou ü•à</li>
						</ul>
					</div>
				</section>

				<section>
					<h3>√âtude sur les EDAs - Bilan</h3>
					<!-- <ol>
						<li><b>EDAs communes :</b> au plus le mieux</li>
						<br>
						<li>Une EDA g√©om√©trique et une EDA en d√©coupage $\Rightarrow$ ‚ûï performances</li>
						<br>
						<li>Int√©r√™t des relations <code>OneOf</code> <span class="ita">(<code>EventDrop</code>, ...)</span></li>
					</ol> -->
					<br>
					<br>
					<p><b>Pour la suite :</b></p>
					<br>
					<p>$D = \{\texttt{Noise,Crop,PolFlip,StatDynGeo,}$ $\texttt{EventCopyDrop}\}$</p>
				</section>

				<section>
					<h3>√âvaluation des Performances</h3>
					<div class="r-stack">
						<ul class="fragment fade-out" data-fragment-index="0">
							<li>R√©sultats des protocoles <b>Semi-supervis√©</b> et <b>Transfert d'Apprentissage</b></li>
							<br>
							<li>Mise en perspective avec les <b>mod√®les supervis√©s</b></li>
							<br>
							<li>Pour un protocole donn√©, on rapporte :</li>
							<ol>
								<li>Le <b>meilleur mod√®le supervis√©</b> de l'√©tat de l'art</li>
								<li>Les <b>deux meilleurs r√©sultats</b> obtenus sur le SSRL √©v√©nementiel</li>
							</ol>
						</ul>
						<div class="r-stack fragment" data-fragment-index="0">
							<img src="these/ssrl/ssrl_benchmarks_0.png" alt="ssrl benchmarks" height="600px" style="margin: 0;">
							<img src="these/ssrl/ssrl_benchmarks_1.png" alt="ssrl benchmarks" class="fragment" height="670px" style="margin:0;">
							<img src="these/ssrl/ssrl_benchmarks_2.png" alt="ssrl benchmarks" class="fragment" height="670px" style="margin:0;">
							<img src="these/ssrl/ssrl_benchmarks_1.png" alt="ssrl benchmarks" class="fragment" height="670px" style="margin:0;">
							<img src="these/ssrl/ssrl_benchmarks_3.png" alt="ssrl benchmarks" class="fragment" height="670px" style="margin:0;">
							<img src="these/ssrl/ssrl_benchmarks_4.png" alt="ssrl benchmarks" class="fragment" height="670px" style="margin:0;">
							<img src="these/ssrl/ssrl_benchmarks_5.png" alt="ssrl benchmarks" class="fragment" height="670px" style="margin:0;">
						</div>
					</div>
				</section>

				<section>
					<h3>Bilan des Contributions</h3>
					<ul>
						<li class="fragment" data-fragment-index="0" style="list-style-type: '‚öôÔ∏è &nbsp;';"> <b>M√©thode de SSRL √©v√©nementielle</b></li>
						<ul class="fragment" data-fragment-index="0">
							<li>Performances comp√©titives voire surpassant les mod√®les supervis√©s</li>
						</ul>
						<br>
						<li class="fragment" data-fragment-index="1" style="list-style-type: 'ü™Ñ &nbsp;';"> <b>√âtude sur les EDAs pour la SSRL</b></li>
						<ul class="fragment" data-fragment-index="1">
							<li>Propositions de nouvelles techniques</li>
						</ul>
						<br>
						<li class="fragment" data-fragment-index="2" style="list-style-type: '‚öñÔ∏è &nbsp;';"> <b>Protocoles d'√©valuation standardis√©s</b></li>
						<ul class="fragment" data-fragment-index="2">
							<li>Fondations pour la SSRL √©v√©nementielle</li>
						</ul>
						<p style="border-top: solid 2px; padding-top: 5px; text-align: justify;"><small>üìî &nbsp; <span class="ita">"Exploring Joint Embedding Architectures and Data Augmentations for Self-Supervised Representation Learning in Event-Based Vision."</span> <b>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023</b>.</small></p>
					</ul>
				</section>
			</section>
			<section>
				<section>
					<h1>Conclusion</h1>
				</section>

				<section>
					<h3>Travaux R√©alis√©s</h3>
					<ul>
						<li class="fragment" data-fragment-index="0" style="text-align: justify; margin:0 20px 20px 0;"><small><span class="ita">"Deep spiking convolutional neural network for single object localization based on deep continuous local learning."</span> <b>CBMI (2021)</b></small></li>
						<li class="fragment" data-fragment-index="0" style="text-align: justify; margin:0 20px 60px 0;"><small><span class="ita">"Spiking neural networks for frame-based and event-based single object localization."</span> <b>Neurocomputing (2023)</b></small></li>
						<li class="fragment" style="text-align: justify; margin:0 20px 60px 0;"><small><span class="ita">"Spiking-Fer: Spiking Neural Network for Facial Expression Recognition With Event Cameras."</span> <b>CBMI (2023)</b></small></li>
						<li class="fragment" style="text-align: justify; margin:0 20px 60px 0;"><small><span class="ita">"Bina-rep event frames: A simple and effective representation for event-based cameras."</span> <b>ICIP (2022)</b></small></li>
						<li class="fragment" style="text-align: justify; margin:0 20px 60px 0;"><small><span class="ita">"Exploring Joint Embedding Architectures and Data Augmentations for Self-Supervised Representation Learning in Event-Based Vision."</span> <b>CVPR Workshop (2023)</b></small></li>
						<!-- <li style="border-top: solid 2px; padding-top: 5px; text-align: justify; color: darkgrey;"><small><span class="ita">"Review on indoor RGB-D semantic segmentation with deep convolutional neural networks."</span> <b>2021 International Conference on Content-Based Multimedia Indexing (CBMI). IEEE, 2021.</b></small></li> -->
					</ul>
				</section>

				<section>
					<h3>Bilan de la Th√®se</h3>
					<ul>
						<li><b>Vision neuromorphique se d√©veloppe rapidement</b></li>
						<ul>
							<li>Importance de l'apprentissage profond (ANN ou SNN)</li>
						</ul>
						<li><b>Domaine encore √† enrichir</b></li>
						<ul style="list-style-type: '&rarr;';">
							<li>&nbsp; Exploration de nouvelles t√¢ches</li>
							<li>&nbsp; Analyses experimentales</li>
							<li>&nbsp; Nouvelles m√©thodes</li>
						</ul>
						<li><b>N√©cessit√©s √† court terme</b></li>
						<ul>
							<li>Confrontation avec d'autres solutions alternatives pour la consommation d'√©nergie $\Rightarrow$ <b>am√©liorer la stabilit√© des m√©thodes</b></li>
						</ul>
					</ul>
				</section>

				<section>
					<h3>Perspectives</h3>
					<ul>
						<li class="fragment" data-fragment-index="0"><b>Mod√®le de fondation pour la vision √©v√©nementielle</b></li>
						<ul class="fragment" data-fragment-index="0">
							<li>Mod√®le massif pr√©-entra√Æn√© par SSRL $\Rightarrow$ <b>polyvalence des t√¢ches de vision</b> </li>
							<li>Distillation du savoir, Few-shot learning, etc.</li>
						</ul>

						<br>

						<li class="fragment" data-fragment-index="1"><b>R√©seaux de neurones binaires √† convolutions √©parses pour la vision √©v√©nementielle</b></li>
						<ul class="fragment" data-fragment-index="1">
							<li>Latence ultra-faible<small>[Chowdhury2021]</small> et √©parsit√© du traitement<small>[Messikomer2020]</small></li>
							<li>Traitement efficace des flux √† comportement statique</li>
						</ul>
					</ul>
				</section>
			</section>

			<section>
				<h3>Merci pour votre attention</h3>
				<p style="text-align: left;"><b>Publications</b> &nbsp; 6Ô∏è‚É£</p>
				<ul>
					<li>Conf√©rences internationales √† comit√© de lecture &nbsp; 5Ô∏è‚É£</li>
					<ul>
						<li><b>CVPR Workshop</b></li>
						<li><b>ICIP</b></li>
						<li><b>CBMI</b></li>
						
					</ul>
					<br>
					<li>Journal international √† comit√© de lecture &nbsp; 1Ô∏è‚É£</li>
					<ul>
						<li><b>Neurocomputing</b></li>
					</ul>
				</ul>
				<br>
				<!-- <p style="text-align: left;"><b>Divers</b></p>
				<ul>
					<li>Encadrements de stages et projets Master &nbsp;  5Ô∏è‚É£</li>
					<li>Enseignement &nbsp;  1Ô∏è‚É£</li>
					<li>ü•á &nbsp; Doctoriales</li>
					<li>üì£ &nbsp; M√©diation scientifique</li>
				</ul> -->
			</section>

			<section>
				<section>
					<h1>R√©f√©rences</h1>
				</section>
				
				<section>
					<!-- <h3>Introduction & √âtat de l'Art</h3> -->
					<ul style="list-style: none; text-align: justify;">
						<li><small><b>[Bokovoy2019]: </b>A. Bokovoy et al. "Real-time Vision-based Depth Reconstruction with NVidia Jetson"</small></li>
						<li><small><b>[Desislavov2023]: </b>R. Desislavov et al. "Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning"</small></li>
						<li><small><b>[Kzh2012]: </b>A. Kzh, et al. "Imagenet classification with deep convolutional neural networks"</small></li>
						<li><small><b>[He2016]: </b>K. He, et al. "Deep residual learning for image recognition"</small></li>
						<li><small><b>[Deng2009]: </b>J. Deng, et al. "Imagenet: A large-scale hierarchical image database"</small></li>
						<li><small><b>[Lapicque1907]: </b>LM Lapicque, "Recherches quantitatives sur l‚Äôexcitation electrique des nerfs"</small></li>
						<li><small><b>[Neftci2019]: </b>E. Neftci et al., "Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks"</small></li>
						<li><small><b>[Li2017]: </b>H. Li et al., "Cifar10-dvs: an event-stream dataset for object classification."</small></li>
						<li><small><b>[K2022]: </b>Y. Kim et al. "Beyond Classification: Directly Training Spiking Neural Networks for Semantic Segmentation"</small></li>
					</ul>
				</section>

				<section>
					<!-- <h3>R√©f√©rences (2)</h3> -->
					<ul style="list-style: none; text-align: justify;">
						<li><small><b>[Crd2022]: </b>L. Cordone et al. "Object Detection with Spiking Neural Networks on Automotive Event Data"</small></li>
						<li><small><b>[Rancon2021]: </b>U. Ran√ßon et al. "StereoSpike: Depth Learning with a Spiking Neural Network"</small></li>
						<li><small><b>[Horowitz2014]: </b>M. Horowitz "Computing's energy problem (and what we can do about it)"</small></li>
						<li><small><b>[Bardes2022]: </b>A. Bardes et al. "VICReg: Variance-Invariance-Covariance Regularization For Self-Supervised Learning"</small></li>
						<li><small><b>[Zbontar2021]: </b>J. Zbontar et al. "Barlow Twins: Self-Supervised Learning via Redundancy Reduction"</small></li>
						<li><small><b>[Fang2021]: </b>W. Fang et al. "Deep residual learning in spiking neural networks"</small></li>
						<li><small><b>[Tran2017]: </b>D. Tran et al. "A Closer Look at Spatiotemporal Convolutions for Action Recognition"</small></li>
						<li><small><b>[Dehghani2023]: </b>M. Dehghani et al. "Scaling Vision Transformers to 22 Billion Parameters"</small></li>
						<li><small><b>[Zhou2022]: </b>Z. Zhou et al. "Spikformer: When Spiking Neural Network Meets Transformer"</small></li>
					</ul>
				</section>

				<section>
					<!-- <h3>R√©f√©rences (2)</h3> -->
					<ul style="list-style: none; text-align: justify;">
						<li><small><b>[Falez2019]: </b>P. Falez "Improving spiking neural networks trained with spike timing dependent plasticity for image recognition"</small></li>
						<li><small><b>[Guo2021]: </b>W. Guo et al. "Neural Coding in Spiking Neural Networks: A Comparative Study for Robust Neuromorphic Systems"</small></li>
						<li><small><b>[Chowdhury2021]: </b>S. S. Chowdhury et al. "One timestep is all you need: Training spiking neural networks with ultra low latency"</small></li>
						<li><small><b>[Messikomer2020]: </b>N. Messikomer et al. "Event-based asynchronous sparse convolutional networks"</small></li>
						<li><small><b>[Gallego2020]: </b>G. Gallego et al. "Event-based vision: a survey"</small></li>
						<li><small><b>[Zheng2023]: </b>X. Zheng et al. "Deep learning for event-based vision: A comprehensive survey and benchmarks"</small></li>
						<li><small><b>[Rudnev2023]: </b>V. Rudnev et al. "EventNeRF: Neural radiance fields from a single colour event camera"</small></li>
						<li><small><b>[Gehrig2023]: </b>M. Gehrig et al. "Recurrent vision transformers for object detection with event cameras"</small></li>
						<li><small><b>[Kielty2023]: </b>P. Kielty et al. "Neuromorphic Driver Monitoring Systems: A Proof-of-Concept for Yawn Detection and Seatbelt State Detection using an Event Camera"</small></li>
					</ul>
				</section>

				<section>
					<ul style="list-style: none; text-align: justify;">
						<li><small><b>[Li2022]: </b>Y. Li et al. "Neuromorphic data augmentation for training spiking neural networks"</small></li>
						<li><small><b>[Klenk2022]: </b>S. Klenk et al. "Masked Event Modeling: Self-Supervised Pretraining for Event Cameras"</small></li>
						<li><small><b>[Yang2023]: </b>Y. Yang et al. "Event Camera Data Pre-training"</small></li>
					</ul>
				</section>
			</section>

			<section>
				<section>
					<h1>Annexes</h1>
					<h4>√âtat de l'Art</h4>
				</section>

				

				<section>
					<h3>R√©seaux de Neurones Impulsionnels</h3>
					<p><b>Apprentissage par Subtitut du Gradient</b> <span class="ita">(SG)</span></p>
					<div class="r-stack">
						<img src="these/sota/if_eq.png" alt="eq" class="fragment fade-in-then-out">
						<img src="these/sota/if_recursive_0.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_recursive_1.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_recursive_2.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_recursive_3.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_recursive_4.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_recursive_5.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_rnn.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_rnn_1.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_forward.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_forward_1.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_forward_2.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_forward_3.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/if_forward_4.png" alt="recu" class="fragment fade-in-then-out" height="550px">
						<img src="these/sota/heaviside.png" alt="recu" class="fragment fade-in-then-out">
						<img src="these/sota/heaviside_back.png" alt="recu" class="fragment fade-in-then-out">
						<img src="these/sota/surrogate.png" alt="recu" class="fragment fade-in-then-out">
						<img src="these/sota/surrogate_back.png" alt="recu" class="fragment fade-in-then-out">
					</div>
				</section>
			</section>

			<section>
				<section>
					<h1>Annexes</h1>
					<h3>Bina-Rep</h3>
				</section>

				<section>
					<h3>Repr√©sentation Bina-Rep</h3>
					<img src="these/binarep/bina1.png" alt="bina">
					<img src="these/binarep/bina2.png" alt="bina">
				</section>

				<section>
					<h3>Visualisations</h3>
					<img src="these/binarep/bina3.png" alt="bina">
				</section>

				<section>
					<h3>Mod√®le ANN</h3>
					<img src="these/binarep/bina4.png" alt="bina">
				</section>

				<section>
					<h3>Comparaison avec Autres Repr√©sentations</h3>
					<img src="these/binarep/bina5.png" alt="bina">
				</section>

				<section>
					<h3>Bases de Donn√©es Utilis√©es</h3>
					<img src="these/binarep/bina6.png" alt="bina">
				</section>

				<section>
					<h3>Comparaison</h3>
					<img src="these/binarep/bina7.png" alt="bina">
				</section>

				<section>
					<h3>Vs SOTA</h3>
					<img src="these/binarep/bina8.png" alt="bina">
				</section>

				<section>
					<h3>Robustesse</h3>
					<img height="600px" src="these/binarep/bina9.png" alt="bina">
				</section>
			</section>

			<section>
				<section>
					<h1>Annexes</h1>
					<h3>Spiking-FER</h3>
					<p><a href="https://barchid.github.io/Presentations/spiking_fer.html" target="_blank">https://barchid.github.io/Presentations/spiking_fer.html</a></p>
				</section>
			</section>

			<section>
				<section>
					<h1>Annexes</h1>
					<h3>Travail Pr√©liminaire de Localisation</h3>
				</section>

				<section>
					<h3>R√®gle d'Apprentissage : DECOLLE</h3>
					<img src="these/decolle/0.png" alt="decolle">
				</section>

				<section>
					<h3>Mod√®le Encodeur-D√©codeur</h3>
					<img src="these/decolle/1.png" alt="decolle">
				</section>

				<section>
					<h3>Calcul de la Pr√©diction</h3>
					<ul>
						<li>$T$ pr√©dictions sur chacune des $L$ couches du r√©seau</li>
						<li>$\Rightarrow$ comment calculer une seule pr√©diction la plus efficace possible ?</li>
						<br>
						<li><b>Intuition utilis√©e : </b>garder la derni√®re pr√©diction (√† l'√©tape $T$) de la derni√®re couche (la couche $L$) $\rightarrow \mathbf{B}^T_L$</li>
						<li style="color: red;">‚ùå &nbsp; Pas de preuve que c'est optimal</li>
					</ul>
				</section>

				<section>
					<h3>R√©sultats sur Oxford-IIIT-Pet</h3>
					<p>$63.2$% mIoU</p>
					<img src="these/decolle/2.png" alt="decolle">
				</section>
			</section>

			<section>
				<section>
					<h1>Annexes</h1>
					<h3>Localisation</h3>
				</section>

				<section>
					<h3>Intersection over Union (IoU)</h3>
					<img height="600px" src="these/loca_annex/iou.png" alt="iou">
				</section>

				<section>
					<ul>
						<li>$\mathcal{L}_{IoU}$ = 1 - IoU</li>
						<li>On calcule les coordonn√©es des centres des boites englobantes $\mathbf{B}$ (pr√©diction) et $\mathbf{B}^{gt}$ (label)</li>
						<ul>
							<li>$\mathbf{b}$ et $\mathbf{b}^{gt}$</li>
						</ul>
						<li>Terme de p√©nalit√© : la distance normalis√©e entre ces deux centres des bbox : $\mathcal{R} = \frac{\rho^2(\mathbf{b},\mathbf{b}^{gt})}{q^2}$</li>
						<ul>
							<li>$\rho(\cdot)$ est la distance euclidienne</li>
							<li>$q$ est la longueur de la diagonale de la plus petite boite qui d√©toure les deux boites englobantes</li>
						</ul>
					</ul>
				</section>

				<section>
					<img src="these/loca_annex/diou.png" alt="diou">
					<p>$\mathcal{L}_{DIoU} = \mathcal{L}_{IoU} + \mathcal{R}$</p>
				</section>

				<section>
					<h3>Images Statiques - Latence</h3>
					<img src="these/localization/latence_full.png" alt="latence full" height="600px">
				</section>

				<section>
					<h3>Consommation √©nerg√©tique</h3>
					<div class="r-stack fragment fade-in" data-fragment-index="0">
						<img src="these/localization/conso_1.png" alt="conso" class="fragment" data-fragment-index="0">
						<img src="these/localization/conso_2.png" alt="conso" class="fragment" data-fragment-index="1">
						<img src="these/localization/conso_3.png" alt="conso" class="fragment" data-fragment-index="2">
						<img src="these/localization/conso.png" alt="conso" class="fragment" data-fragment-index="3">
						<img src="these/localization/conso_4.png" alt="conso" class="fragment" data-fragment-index="4">
						<img src="these/localization/conso_5.png" alt="conso" class="fragment" data-fragment-index="5">
					</div>
				</section>

				<section>
					<h3>Image Statique - Evolution $RAD^{corr}_{sev}$</h3>
					<img src="these/loca_annex/rad_evo.png" alt="rad evo">
				</section>
				<section>
					<h3>√âv√©nements - Evolution $RAD^{corr}_{sev}$</h3>
					<img src="these/loca_annex/rad_event_evo.png" alt="rad evo">
				</section>

				<section>
					<h3>Fr√©quences d'Activations des SNNs</h3>
					<img src="these/loca_annex/rate_snn.png" alt="rate">
				</section>

				<section>
					<h3>Latences √âv√©nements</h3>
					<img src="these/loca_annex/latence_events.png" alt="latences">
				</section>
			</section>

			<section>
				<section>
					<h1>Annexes</h1>
					<h4>SSRL √âv√©nementiel</h4>
				</section>


				<section>
					<h3>VICReg</h3>
					<img height="230px" src="these/vicreg_annexe.png" alt="vicreg annexe">
					<ol>
						<li><b>Invariance : </b>minimiser la distance entre les deux encastrements de la m√™me entr√©e</li>
						<li><b>Variance : </b>maintenir la variance de chaque variable d'un m√™me vecteur dans un lot au-dessus d'un seuil</li>
						<li><b>Covariance : </b>minimiser la covariance entre les valeurs d'un m√™me vecteur</li>
					</ol>
				</section>

				<section>
					<h3>Augmentations de Donn√©es √âtudi√©es</h3>
					<div class="r-stack">
						<p class="fragment fade-in-then-out" data-fragment-index="0"><b>Exemple</b></p>
						<p style="border: solid 3px  blue; margin:10px; color:blue;" class="fragment fade-in-then-out" data-fragment-index="1"><b>Augmentations Communes</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="2"><b>Augmentations Communes</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="3"><b>Augmentations Communes</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="4"><b>Augmentations Communes</b></p>

						<p style="border: solid 3px blue; margin:10px; color:blue;" class="fragment fade-in-then-out" data-fragment-index="5"><b>Augmentations en D√©coupage</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="6"><b>Augmentations en D√©coupage</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="7"><b>Augmentations en D√©coupage</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="8"><b>Augmentations en D√©coupage</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="9"><b>Augmentations en D√©coupage</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="10"><b>Augmentations en D√©coupage</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="11"><b>Augmentations en D√©coupage</b></p>

						<p style="border: solid 3px blue; margin:10px; color:blue;" class="fragment fade-in-then-out" data-fragment-index="12"><b>Augmentations G√©om√©triques</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="13"><b>Augmentations G√©om√©triques</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="14"><b>Augmentations G√©om√©triques</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="15"><b>Augmentations G√©om√©triques</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="16"><b>Augmentations G√©om√©triques</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="17"><b>Augmentations G√©om√©triques</b></p>
					</div>
					<div class="r-stack">
						<img src="these/eda/normal.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="0">
						<p class="fragment fade-in-then-out" data-fragment-index="1">üìñ &nbsp; Transformations couramment utilis√©es, ne partagent <b>pas de caract√©ristiques communes</b>.</p>
						<img src="these/eda/background_activity.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="2">
						<img src="these/eda/polarity_flip.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="3">
						<img src="these/eda/crop.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="4">

						<p class="fragment fade-in-then-out" data-fragment-index="5">üìñ &nbsp; Transformations impliquant la <b>suppression d'√©v√©nements</b>.</p>
						<img src="these/eda/cutout.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="6">
						<img src="these/eda/drop_by_time.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="7">
						<img src="these/eda/random_drop.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="8">
						<img src="these/eda/event_drop.png" height="400px" alt="normal" class="fragment fade-in-then-out" data-fragment-index="9">
						<img src="these/eda/event_copy.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="10">
						<img src="these/eda/event_copy_drop.png" height="400px" alt="normal" class="fragment fade-in-then-out" data-fragment-index="11">

						<p class="fragment fade-in-then-out" data-fragment-index="12">üìñ &nbsp; Transformations impliquant une <b>distorsion spatiale des √©v√©nements</b>.</p>
						<img src="these/eda/static_translation.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="13">
						<img src="these/eda/static_rotation.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="14">
						<img src="these/eda/dynamic_translation.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="15">
						<img src="these/eda/dynamic_rotation.gif" alt="normal" class="fragment fade-in-then-out" data-fragment-index="16">
						<img src="these/eda/stat_dyn_geo.png" height="400px" alt="normal" class="fragment fade-in-then-out" data-fragment-index="17">

					</div>
					<div class="r-stack">
						<p class="fragment fade-in-then-out" data-fragment-index="0"></p>
						<p class="fragment fade-in-then-out" data-fragment-index="1"></p>
						<p class="fragment fade-in-then-out" data-fragment-index="2">Bruit d'activit√© de fond (<code>Noise</code>)</p>
						<p class="fragment fade-in-then-out" data-fragment-index="3">Inversion de polarit√© (<code>PolFlip</code>)</p>
						<p class="fragment fade-in-then-out" data-fragment-index="4">Recadrage (<code>Crop</code>)</p>
						<!-- decoupage-->
						<p class="fragment fade-in-then-out" data-fragment-index="5"></p>
						<p class="fragment fade-in-then-out" data-fragment-index="6">D√©coupe par zone (<code>Cutout</code>)</p>
						<p class="fragment fade-in-then-out" data-fragment-index="7">D√©coupe par dur√©e</p>
						<p class="fragment fade-in-then-out" data-fragment-index="8">D√©coupe al√©atoire</p>
						<p class="fragment fade-in-then-out" data-fragment-index="9"><code>EventDrop</code></p>
						<p class="fragment fade-in-then-out" data-fragment-index="10">üÜï &nbsp; <code>EventCopy</code></p>
						<p class="fragment fade-in-then-out" data-fragment-index="11">üÜï &nbsp; <code>EventCopyDrop</code></p>
						<!-- geo -->
						<p class="fragment fade-in-then-out" data-fragment-index="12"></p>
						<p class="fragment fade-in-then-out" data-fragment-index="13">Translation statique (<code>StatTran</code>)</p>
						<p class="fragment fade-in-then-out" data-fragment-index="14">Rotation statique (<code>StatRot</code>)</p>
						<p class="fragment fade-in-then-out" data-fragment-index="15">üÜï &nbsp; Translation dynamique (<code>DynTran</code>)</p>
						<p class="fragment fade-in-then-out" data-fragment-index="16">üÜï &nbsp; Rotation dynamique (<code>DynRot</code>)</p>
						<p class="fragment fade-in-then-out" data-fragment-index="17">üÜï &nbsp; <code>StatDynGeo</code></p>
					</div>
				</section>

				<section>
					<h3>Distribution EDAs</h3>
					<img src="these/params_edas.png" alt="param edas" height="600px">
				</section>

				<section>
					<h3>√âtude sur les EDAs</h3>
					<p><b>R√©sultats</b></p>
					<div class="r-stack">
						<img alt="tab result edas" src="these/results_edas/tab_0.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_1.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_2.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_3.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_4.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_5.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_6.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_7.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_8.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_9.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_10.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_11.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_12.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_13.png">
						<img alt="tab result edas" class="fragment" src="these/results_edas/tab_14.png">
					</div>
				</section>

				<section>
					<h3>√âvaluation des Performances</h3>
					<div class="r-stack">
						<p class="fragment fade-in-then-out" data-fragment-index="0"><b>√âvaluation Lin√©aire et Apprentissage Semi-supervis√©</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="1"><b>√âvaluation Lin√©aire et Apprentissage Semi-supervis√©</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="2"><b>√âvaluation Lin√©aire et Apprentissage Semi-supervis√©</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="3"><b>√âvaluation Lin√©aire et Apprentissage Semi-supervis√©</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="4"><b>√âvaluation Lin√©aire et Apprentissage Semi-supervis√©</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="5"><b>√âvaluation Lin√©aire et Apprentissage Semi-supervis√©</b></p>

						<p class="fragment fade-in-then-out" data-fragment-index="6"><b>Transfert d'Apprentissage</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="7"><b>Transfert d'Apprentissage</b></p>
						<p class="fragment fade-in-then-out" data-fragment-index="8"><b>Transfert d'Apprentissage</b></p>
					</div>
					<div class="r-stack">
						<img src="these/results_ssl/tab_0.png" alt="results ssl" class="fragment fade-in-then-out" data-fragment-index="0">
						<img src="these/results_ssl/tab_1.png" alt="results ssl" class="fragment fade-in-then-out" data-fragment-index="1">
						<img src="these/results_ssl/tab_2.png" alt="results ssl" class="fragment fade-in-then-out" data-fragment-index="2">
						<img src="these/results_ssl/tab_3.png" alt="results ssl" class="fragment fade-in-then-out" data-fragment-index="3">
						<img src="these/results_ssl/tab_4.png" alt="results ssl" class="fragment fade-in-then-out" data-fragment-index="4">
						<img src="these/results_ssl/tab_5.png" alt="results ssl" class="fragment fade-in-then-out" data-fragment-index="5">

						<img src="these/results_ssl/transfer_0.png" alt="results ssl" class="fragment fade-in-then-out" data-fragment-index="6">
						<img src="these/results_ssl/transfer_1.png" alt="results ssl" class="fragment fade-in-then-out" data-fragment-index="7">
						<img src="these/results_ssl/transfer.png" alt="results ssl" class="fragment fade-in-then-out" data-fragment-index="8">
					</div>
					<div class="r-stack">
						<p class="fragment fade-in-then-out" data-fragment-index="0"></p>
						<p class="fragment fade-in-then-out" data-fragment-index="1"></p>
						<p class="fragment fade-in-then-out" data-fragment-index="2"></p>
						<p class="fragment fade-in-then-out" data-fragment-index="3"></p>
						<p class="fragment fade-in-then-out" data-fragment-index="4">2D-/3D-CNNs $>$ CSNN</p>
						<p class="fragment fade-in-then-out" data-fragment-index="5">Int√©r√™t de la variante "√âtudiant-Professeur"</p>
						<p class="fragment fade-in-then-out" data-fragment-index="6"></p>
						<p class="fragment fade-in-then-out" data-fragment-index="7"></p>
						<p class="fragment fade-in-then-out" data-fragment-index="8">‚úÖ &nbsp; transf√©rabilit√© des repr√©sentations apprises</p>
					</div>
				</section>

				<section>
					<h3>Mise en Perspective - SSRL √©v√©nementiel</h3>
					<p><b>ASL-DVS</b></p>
					<img height="500px" src="these/results_ssl/perspective_asl.png" alt="ASL">
				</section>

				<section>
					<h3>Mise en Perspective - SSRL √©v√©nementiel</h3>
					<p><b>N-CARS</b></p>
					<img height="500px" src="these/results_ssl/perspective_ncars.png" alt="ASL">
				</section>

				<section>
					<h3>Mise en Perspective - SSRL √©v√©nementiel</h3>
					<p><b>N-CALTECH101</b></p>
					<img src="these/results_ssl/perspective_ncaltech101.png" alt="ASL">
				</section>

				<section>
					<h3>Mise en Perspective - SSRL √©v√©nementiel</h3>
					<p><b>DVSGesture</b></p>
					<img src="these/results_ssl/perspective_dvsgesture.png" alt="ASL">
				</section>

				<section>
					<h3>Mise en Perspective - SSRL √©v√©nementiel</h3>
					<p><b>DailyAction-DVS</b></p>
					<img src="these/results_ssl/perspective_dailyactiondvs.png" alt="ASL">
				</section>

				<section>
					<h3>Analyse - Uniformit√© et Tol√©rance</h3>
					<ul>
						<li>Mesures calcul√©es sur l'ensemble des repr√©sentations sur une m√™me BDD</li>
						<li><b>Uniformit√© :</b> mesurer la proximit√© de la distribution des repr√©sentations sur l'hypersph√®re des caract√©ristique avec une distribution uniforme</li>
						<li><b>Tol√©rance :</b> mesurer dans quelle mesure les repr√©sentations capturent les relations s√©mantiques entre les √©chantillons</li>
						<br>
						<li><b>En SSRL classique : </b> un √©quilibre de ces deux valeurs $\Rightarrow$ meilleures repr√©sentations</li>
					</ul>
				</section>
				<section>
					<img src="these/ssl_annex/uf_cal.png" alt="cal">
					<img src="these/ssl_annex/uf.png" alt="uf">
				</section>


				<section>
					<h3>üîé &nbsp; Similarit√© des Repr√©sentations</h3>
					<ul>
						<li><b>CKA Lin√©aire :</b></li>
						<ul>
							<li>Utilis√©e en SSRL pour les images</li>
							<li>Compare les repr√©sentations de <b>deux encodeurs</b></li>
							<li>Donne une valeur $\in [0,1]$ √©valuant leur similarit√©</li>
						</ul>
						<br>
						<li><b>üéØ &nbsp; Nos objectifs :</b></li>
						<ul>
							<li>Comparer tous les encodeurs entre eux...</li>
							<li>... selon chaque bloc r√©siduel</li>
							<li><b>Base de donn√©es :</b> DVSGesture</li>
						</ul>
					</ul>
				</section>

				<section>
					<h3>üîé &nbsp; Similarit√© des Repr√©sentations</h3>
					<div class="r-stack">
						<img src="these/results_ssl/cka_0.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_1.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_2.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_3.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_4.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_5.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_6.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_7.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_8.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_9.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_10.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_11.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_12.png" alt="cka" class="fragment">
						<img src="these/results_ssl/cka_13.png" alt="cka" class="fragment">
					</div>
				</section>

				<section>
					<h3>Limitations</h3>
					<ul>
						<li><b>In√©galit√©s des encodeurs</b></li>
						<ul>
							<li>‚ùå &nbsp; $CSNN < 2D\text{ et }3D$</li>
						</ul>
						<br>
						<li>Protocoles d'√©valuation bas√©s sur la <b>classification uniquement</b></li>
						<ul>
							<li>‚ùå &nbsp; Information spatiale</li>
						</ul>
						<br>
						<li>Les caract√©ristiques sont extraites sur l'ensemble des √©v√©nements</li>
						<ul>
							<li>‚ùå &nbsp; Traitement de l'information spatio-temporel</li>
						</ul>
					</ul>
					<p style="border-top: solid 2px; padding-top: 5px; text-align: justify;"><small>üìî &nbsp; <span class="ita">"Exploring Joint Embedding Architectures and Data Augmentations for Self-Supervised Representation Learning in Event-Based Vision."</span> <b>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023</b>.</small></p>
				</section>
			</section>
		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script src="plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,
			slideNumber: 'c',
			math: {
				mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				config: 'TeX-AMS_HTML-full',
				// pass other options into `MathJax.Hub.Config()`
				TeX: { Macros: { RR: "{\\bf R}" } }
			},
			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
		});
	</script>
</body>

</html>